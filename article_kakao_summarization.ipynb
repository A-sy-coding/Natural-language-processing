{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "article_kakao_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1X_gO6w1ZFsG46Uht1yNS-UdlOUhtnZLq",
      "authorship_tag": "ABX9TyMOnWBM33v7itSCKoI/FFgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-sy-coding/Natural-language-processing/blob/main/article_kakao_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V20I4YlVk3jG"
      },
      "source": [
        "# 크롤링 함수 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGiV7vZSlCC9"
      },
      "source": [
        "# 필요한 라이브러리 \n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4.element\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLSqWhCAlCB9"
      },
      "source": [
        "# beautifulsoup 객체 생성\n",
        "# header 매개변수 변경 --> requests의 get 메소드에는 headers 매개변수가 존재\n",
        "\n",
        "#=> http://www.useragentstring.com에서 지금 접속하는 브라우저의 header정보를 복사하여 python으로 가져옴\n",
        "\n",
        "def get_soup_obj(url):\n",
        "\n",
        "  headers = {'User-Agent' : 'Mozilla/5.0'}\n",
        "  res = requests.get(url, headers = headers)\n",
        "  soup = BeautifulSoup(res.text,'html.parser')  # html소스 가져오기\n",
        "                                                 # 첫인자는 html소스, 두번째 인자는 어떤 parser를 이용할지 명시\n",
        "  return soup                                                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqmOPEY5lCBI"
      },
      "source": [
        "# 뉴스의 기본 정보 가져오기\n",
        "\n",
        "def get_top3_news_info(sec):\n",
        "\n",
        "  # 상위 뉴스 목록 주소\n",
        "  sec_url = 'https://news.naver.com/main/home.naver'\n",
        "\n",
        "  # 상위 뉴스 HTML 가져오기\n",
        "  soup = get_soup_obj(sec_url)\n",
        "\n",
        "  # 상위 뉴스 3개 가져오기\n",
        "  news_list = []\n",
        "\n",
        "      # div 태그에서 class, id속성값에 해당되는 값들을 구하고, 그 중에서 li 태그를 가진 것들만 고른다.\n",
        "  news = soup.find(\"div\", attrs={'class':'main_component droppable', 'id':'section_'+sec}).find_all('li', limit=3)\n",
        "  \n",
        "  for li in news:\n",
        "    # title : 뉴스 제목, news_url : 뉴스 URL, image_url : 이미지 URL\n",
        "    news_info = {\n",
        "            \"title\" : li.find('strong').text if li.img else li.a.text.replace(\"\\n\", \"\").replace(\"\\t\",\"\").replace(\"\\r\",\"\") , \n",
        "            \"news_url\" : li.a.attrs.get('href')}\n",
        "    news_list.append(news_info)\n",
        "    \n",
        "  return news_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aidOVYV4lB7m"
      },
      "source": [
        "# 뉴스 이미지/본문 가져오기\n",
        "\n",
        "def get_news_contents(url):\n",
        "\n",
        "  soup = get_soup_obj(url)  # html 가져오기\n",
        "  body = soup.select(\"div._article_body_contents\")[0]\n",
        "\n",
        "  # 임시 이미지 ( 기사에 사지이 없을 경우 )\n",
        "  default_img = \"https://search.naver.com/search.naver?where=image&sm=tab_jum&query=naver#\"\n",
        "  try:\n",
        "    news_image = soup.find('span', attrs={'class':'end_photo_org'}).img.attrs.get('src')\n",
        "  except:\n",
        "    news_image = default_img\n",
        "\n",
        "  news_contents = ''\n",
        "  for content in body:\n",
        "      if type(content) is bs4.element.NavigableString and len(content) > 50:\n",
        "          news_contents += content.strip() + ' '    # 뉴스 요약을 위하여 '.' 마침표 뒤에 한칸을 띄워 문장을 구분하도록 함\n",
        "    \n",
        "  return news_image, news_contents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi246ZnAlB0Q"
      },
      "source": [
        "# 정치, 경제, 사회 분야의 상위 3개 뉴스 크롤링\n",
        "\n",
        "def get_naver_news_top3():\n",
        "\n",
        "  news_dic = dict()\n",
        "\n",
        "  # selections -> 정치, 경제, 사회\n",
        "  sec = ['pol','eco','soc']\n",
        "  sections = [\"politics\", \"economy\",\"society\"]\n",
        "\n",
        "  for sec, sections in zip(sec, sections):\n",
        "\n",
        "    # 뉴스의 기본 정보 가져오기\n",
        "    news_info = get_top3_news_info(sections)\n",
        "\n",
        "    for news in news_info:\n",
        "\n",
        "      # 뉴스 본문 가져오기\n",
        "      news_url = news['news_url']  # 딕셔너리 형태로 저장되어 있는 news_url 가져오기\n",
        "      news_image, news_contents = get_news_contents(news_url)\n",
        "      \n",
        "      # 뉴스 정보를 저장하는 dictionary를 구성\n",
        "      news['image_url'] = news_image\n",
        "      news['news_contents'] = news_contents\n",
        "\n",
        "    news_dic[sec] = news_info\n",
        "    \n",
        "  return news_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGMlezFXlBzI",
        "outputId": "cbdaa6c2-1a5e-4814-c558-65f49168d59a"
      },
      "source": [
        "# '정치', '경제', '사회' 분야의 상위 3개 뉴스 크롤링\n",
        "news_dic = get_naver_news_top3()\n",
        "news_dic['pol'],news_dic['eco'],news_dic['soc']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'image_url': 'https://imgnews.pstatic.net/image/016/2021/09/05/20210905000163_0_20210905155004117.jpg?type=w647',\n",
              "   'news_contents': '[헤럴드경제=정윤희 기자]이준석 국민의힘 대표는 5일 “정홍원 당 선거관리위원장이 많은 고생을 하고 계시기 때문에 더 큰 성원과 지지, 신뢰를 보낸다는 말씀을 드린다”고 정 위원장에 힘을 실었다. 경선룰을 둘러싼 갈등이 첨예해지며 일부 대선주자들이 당 공식행사에 불참하고, 정 위원장의 사의설까지 나온 것을 염두에 둔 것으로 풀이된다. 이 대표는 이날 서울 여의도 중앙당사에서 열린 대선주자 간담회 및 경선후보 공정경선 서약식에서 홍준표 의원과 유승민 전 의원 등을 겨냥해 “오늘 우리 경선에 서막을 알리는 공정선거 서약 자리에 빠진 자리들이 있는 거 같아서 당 대표로서 매우 유감”이라며 이같이 말했다. 이날 홍 의원과 유 전 의원, 하태경 의원, 안상수 전 인천시장 등 대선주자 4명은 앞서 예고한대로 공정경선 서약식에 불참했다. 이들은 역선택 방지 조항을 도입 않기로 한 경선준비위원회 안을 확정하라고 요구하고 있다. 박찬주 예비역 대장은 당초 ‘보이콧 입장문’에 이름을 올렸지만, 이날 행사에 참석했다. 이 대표는 이들을 겨냥해 “당 선거관리에 전권을 부여받은 선관위의 운영에 다소간의 불만이 있다고 해서 당 공식행사에 불참하는 행위에 대해서 매우 우려스럽고 다시 반복돼선 안 된다는 생각하게 된다”며 “앞으로 주자들 경우에 다소간 이견 있다하더라도 성숙한 방식으로 본인들의 의사를 표현하고 최소한 선관위에 대한 기본적 예의를 지켜야 된다”고 했다. 그러면서 “당 대표로서 말하지만 지난 2012년 총선을 승리로 이끄셨던 공관위원장이셨던, 우리 정부에서 존경받는 총리 역임하신 정홍원 선관위원장께선 지도부에 무한한 신임과 지지를 받고 계신다”며 “항상 우리 당에 어려울 때마다 많은 도움 주시는 정 총리께 당 대표로서 감사하다는 말씀을 드린다”고 힘을 실었다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=100&oid=016&aid=0001883465',\n",
              "   'title': '이준석, 불참한 洪·劉 겨냥 “매우 유감…선관위에 예의 지켜야”'},\n",
              "  {'image_url': 'https://imgnews.pstatic.net/image/008/2021/09/05/0004640938_001_20210905154302528.jpg?type=w647',\n",
              "   'news_contents': '정홍원 국민의힘 선거관리위원장이 \\'역선택 방지 문항\\' 논란과 관련해 \"어디에 치우친 게 아니고 민주적 의사에 따라 하려고 한다\"며 \"일방적으로 누구를 유리하게 하려고 한다는 선입견을 갖지 말라\"고 당부했다. 자신의 사의 표명 보도에 대해선 언급하지 않았다. 정 위원장은 5일 서울 여의도 국민의힘 당사에서 역선택 방지 문항 논의 절차에 대해 구체적으로 설명했다. 그는 \"후보들의 찬반 의견을 듣고 찬성 2명, 반대 2명, 중립 2명 전문가를 모시고 의견을 청취했다\"며 \"핵심 요지는 \\'역선택 우려는 있다, 그러나 과학적으로 증명되지 않았다\\'다\"고 말했다. 그러면서 \"위원들이 장시간 토론한 결과 2가지 안으로 압축됐다. 하나는 방지 문항을 두지 않는 안이고, (다른 안은) 역선택 문항이 있는 여론조사와 없는 여론조사 결과를 합산하는 두 개의 안으로 압축됐다\"며 \"이것에 대한 논의 결과는 반반이었다\"고 말했다. 정 위원장은 \"결론을 내지 못하고 주말에 시간을 갖고 결론을 내기로 한 상황이다\"고 강조했다. 이날 서약식에는 역선택 방지 문항에 반대한 홍준표 의원과 유승민 전 의원 등이 불참했다. 이준석 국민의힘 대표는 강한 유감을 표명했다. 이 대표는 \"오늘 선관위에 다소간 불만이 있다고 해서 불참한 행위에 대해선 매우 우렵스럽고 다시는 반복돼선 안 된다\"며 \"이견이 있더라도 성숙한 방식으로 본인들의 의사를 표명하고 최소한 선관위에 대한 기본적인 예의를 지켜야 한다고 말씀드린다\"고 말했다. 그러면서 \"다시 한 번 공정선거 서약식에 모든 후보들이 오지 못한 것에 유감이다\"고 지적했다. 이 대표는 정 위원장을 향해 \"지도부는 무한한 신뢰와 지지를 보내고 있다\"며 \"더 큰 성원과 지지, 신뢰를 보낸다\"고 말했다. 서약식에 앞서 정 위원장이 이 대표에게 사의를 표명했다는 보도를 의식한 발언으로 풀이된다. 장성민 세계와동북아평화포럼 이사장이 발언 도중 사의 표명 보도에 대해 묻자 이 대표를 한 손을 가로젓기도 했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=100&oid=008&aid=0004640938',\n",
              "   'title': '이준석 \"무한 지지\"…정홍원, \\'역선택 갈등\\'에 \"선입견 갖지 말라\" 일침'},\n",
              "  {'image_url': 'https://imgnews.pstatic.net/image/469/2021/09/05/0000627843_001_20210905154123815.jpg?type=w647',\n",
              "   'news_contents': '여야 대선주자들이 윤석열 전 검찰총장의 \\'고발 사주\\' 의혹을 겨냥한 총공세를 펼치고 있다. 윤 전 총장은 \\'정치 공작\\'이라며 해명하고 있지만, 납득이 가지 않는다고 반응하면서다. 여당 주자들에게는 본선에서 맞붙을 가능성이 가장 큰 상대에 대한 견제 목적이, 야당 주자들에게는 야권 1위 후보 추격을 위한  동력을 확보하고 존재감을 키우려는 계산이 깔려 있다. 더불어민주당 대선주자인 이재명 경기지사는 5일 대구상공회의소에서 가진 기자간담회에서 윤 전 검찰총장에 대해 \"검찰권을 사적으로 남용하는 데 개입했다는 의혹이 지금 계속 나오고 있다\"며 \"본인이 적폐 그 자체였던 것 같다\"고 직격했다. 이어 \"진실이 아니길 바라지만, 알고도 방치했다면 민주주의 질서 자체를 위협하는 국정농단 그 자체이고 본인이 청산돼야 할 적폐 세력 자체\"라고도 했다. \\'추-윤 갈등\\'의 당사자인 추미애 전 법무부 장관도 이날 페이스북에 윤 전 총장 측이 해당 의혹에 \"증거를 대라\"고 대응한 것을 지적하며 \"일국의 검찰총장까지 지낸 분의 언사로는 대단히 부적절해 보인다\"고 비판했다. 그러면서 \"궁지에 몰린 범죄자들이 뭔가 두려운 장래를 직감하고 마지막 순간에 입에 다는 언사\"라고 했다. 추 전 장관은 \"윤석열은 이제 더이상 무소불위의 검찰총장 신분이 아니고 깨알 검증을 피할 수 없는 대권후보\"라고 했다. 윤 전 총장 캠프에서 \\'추미애 사단의 정치공작\\'이라 주장한 것에는 \"황당한 말을 난사한다\"며 일축했다. 국민의힘 내 경쟁주자인 홍준표 의원은 4, 5일 페이스북에 윤 전 총장을 비판하는 메시지를 잇따라 올렸다. 그는 4일 \"참 보기가 딱하다. 수사 공작은 간첩 잡는 대공 수사 때나 하는 것이다. \\'증거 내놔라\\' 식의 우격다짐만으로는 수습이 안 될 것 같다\"고 꼬집었다. 5일에는 \"곧 드러날 일을 공작정치 운운으로 대응하는 것은 기존 정치인들이 통상 하는 무조건 부인하고 보자는 배 째라식 후안무치 대응\"이라며 \"이제 진실게임에 들어가 버려 일이 커질 대로 커졌다\"고 지적했다. 그러면서 \"지금이라도 진실을 고백하고 대국민 사과를 하시라\"고 했다. 유승민 전 의원도  4일 페이스북에 \"윤 전 총장 의혹이 만약 사실이라면, 이는 검찰총장의 공권력을 사유화한 헌법 유린 범죄\"라고 주장했다. 특히 윤 전 총장에게 ①고발 사주 의혹 관련 서류 작성과 전달 과정을 알고 있었는지 ②관여·지시한 사실이 드러나면 후보직 사퇴 여부에 대한 입장을 국민 앞에서 밝힐 것을 촉구했다. 윤 전 총장은 지난해 4월 21대 총선에 앞서 자신의 최측근 검사가 국민의힘에 범여권 정치인에 대한 고발을 사주했다는 의혹을 받고 있다. 윤 전 총장은 사실무근이라고 주장하고 있지만, 당내에서조차 경선을 앞두고 논란이 수그러들 기미가 보이지 않고 있다. 윤 전 총장을 겨냥한 공세가 확산되고 있지만, 국민의힘 지도부는 적극적인 엄호보다는  신중한 태도를 보이고 있다. 이준석 대표는 이날  KBS인터뷰에서 \"당무 감사를 통해 살펴보겠지만, 당무 감사의 범위는 굉장히 좁다\"며 \"검찰에서 빨리 감찰을 통해 결론을 내려야 한다\"고만 말했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=100&oid=469&aid=0000627843',\n",
              "   'title': '\"적폐 자체\" \"진실 밝혀라\"... 윤석열 \\'고발 사주\\' 의혹에 여야 주자들 총공세'}],\n",
              " [{'image_url': 'https://imgnews.pstatic.net/image/018/2021/09/05/0005029757_001_20210905154603798.jpg?type=w647',\n",
              "   'news_contents': '[세종=이데일리 원다연 기자] 내년 공무원·군인·국민·사학연금 등 4대 공적연금의 적자 보전 등을 위해 투입되는 국가재정이 8조7000억원에 달할 전망이다. 저출산과 고령화로 연금을 받는 수급자가 급증하면서 눈덩이처럼 적자가 불어나고 있어 미래세대를 고려한 연금 개혁이 시급하다는 지적이 나온다. 5일 기획재정부가 국회에 제출한 `2021~2025년 국가재정 운용계획`에 따르면 적자 보전과 사용자 부담금 등으로 지출되는 4대 공적연금에 대한 국가부담 규모는 내년 8조7106억원에서 2023년 9조2750억원, 2024년 9조8114억원으로 늘어난 뒤 2025년이 되면 10조4381억원까지 불어날 전망이다. 국가부담 규모가 늘어나는 주된 이유는 수입보다 지출이 많은 적자 때문이다. 내년에 공무원·군인연금은 각각 3조730억원, 2조9077억원의 적자를 기록한다. 사학연금은 2023년부터 적자로 전환한다. 흑자인 국민연금을 제외하면 공무원·군인·사학연금의 재정수지 적자폭은 2023년 8조9128억원, 2024년 9조6832억원, 2025년 11조2498억원으로 급증한다. 국민연금도 안심할 수 없다. 갈수록 연금 적자 규모가 커지고 있어서다. 국회예산정책처가 지난해 내놓은 4대 공적연금 장기전망에 따르면 오는 2050년에는 공무원연금과 군인연금, 사학연금의 적자 규모가 각각 17조2000억원, 4조2000억원, 2조5000억원 수준으로 불어난다. 예정처는 국민연금 역시 오는 2040년이면 14조1000억원 규모의 적자로 전환할 것으로 봤다. 윤석명 한국보건사회연구원 연구위원은 “현재 공무원·군인연금 등 공적연금 체계는 평균수명, 경제여건 변화 등에 따라 연금 지급이 달라지는 자동안전장치가 없다”며 “연금을 받는 사람보다 내는 사람이 훨씬 많았던 당시 세대간 연대 원리에 따라 설계된 구조를 현재 상황에 맞게 조정하는 것이 필요하다”고 말했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=101&oid=018&aid=0005029757',\n",
              "   'title': '내년 4대 연금에 8.7兆 세금 붓는다…\"개혁 서둘러야\"'},\n",
              "  {'image_url': 'https://imgnews.pstatic.net/image/008/2021/09/05/0004640936_001_20210905154102528.jpg?type=w647',\n",
              "   'news_contents': '\"전세계 부유식 해상풍력발전에서 최초와 최대 타이틀을 보유하고 있는 기업으로서 한국을 아시아 최고의 해상풍력 클러스터(집적단지) 후보지로 판단하고 사업을 진행 중입니다. 한국은 해상풍력발전에 성공할 수 있는 모든 제반 조건을 갖추고 있습니다.\" 노르웨이 국영 종합 에너지기업 에퀴노르의 한국 지부를 이끌고 있는 쟈크 엔티엔 미셸 에퀴노르코리아 지사장은 지난 2일 머니투데이와 만난 자리에서 부유식 해상풍력 최강자인 에퀴노르가 한국을 사업지로 선택한 이유에 대해 이같이 역설했다. 1972년 설립돼 전 세계 30여 개국에서 40여년간 석유·가스를 개발하고 공급해온 에퀴노르는 기후 변화에 문제의식을 가지면서 10여 년 전부터 풍력·태양광 등 신재생에너지 개발에 눈을 돌려왔다. 특히 부유식 해상풍력발전은 에퀴노르의 전문 분야다. 에퀴노르는 이미 한국 울산에서 800MW(메가와트) 규모 반딧불 부유식 해상풍력발전 사업과 200MW 규모 동해1 부유식 해상풍력발전 사업 개발을 맡고 있다. 신재생에너지 사업 전략통으로 꼽히는 미셸 지사장이 한국 해상풍력발전의 사업성을 검토하기 위해 한국에 처음 발령을 받아 온 것은 2018년이다. 그는 당시 한국이 해상풍력발전 적임지란 판단을 내렸고 2019년 한국지부에 정식으로 부임했다. 미셸 지사장이 해상풍력발전의 성공 요건으로 꼽은 것은 5가지다. △정부의 확고한 의지 △대규모 해상풍력발전 가능 여부 △해상풍력발전 공급망 △충분히 많은 전력 소비량 △연평균 풍속·풍속 분포 등 적당한 풍황자원 등이다. 에퀴노르는 한국이 이 모든 걸 갖췄다고 확신했다. 미셸 지사장은 \"해상풍력발전을 추진하려는 (정부) 당국의 의지가 가장 중요한데 한국은 의지도 확고하고, 해상풍력 규모를 크게 키울 수 있는 환경\"이라며 \"해상풍력발전 구조물 제작 역량을 가진 업체도 많아 공급망도 잘 갖춰져 있다\"고 평가했다. 이어 \"한국은 전력 소비량도 독일과 프랑스와 비슷하다\"며 \"정부 지원 의지가 있고 역량도 있어 신재생에너지 사업성이 있는 국가\"라고 말했다. 한국은 지난해 8월 세계 5대 해상풍력 강국으로 도약하겠다는 \\'해상풍력 발전방안\\'을 발표하고 2030년까지 해상풍력 12GW(기가와트)를 보급하겠다는 목표를 내놨다. 미셸 지사장은 \"한국 입장에선 새로운 산업을 키울 수 있는 좋은 기회\"라며 \"한국이 부유식 해상풍력 리더로 자리잡으면 한국 공급업체들이 노하우 갖고 다른 나라에 수출할 수 있다\"고 설명했다. 에퀴노르는 정부가 이 목표를 달성하기 위해선 부유식 해상풍력발전이 반드시 필요하다고 분석했다. 부유식 해상풍력은 보통 해안에서 30~70km 떨어진 바다에 설치된다. 육지에서 먼 바다는 풍속도 높고 안정적이다. 미셸 지사장은 \"수심이 80m 이상인 깊은 바다에 전 세계 해상풍력 자원 80% 이상이 몰려있다\"며 \"수심 깊은 바다에서 풍력발전을 하려면 땅에 고정된 고정식이 아니라 부유식 풍력발전을 해야 한다\"고 강조했다. 실제로 수심이 깊은 한국 바다 특성과 잘 맞을 뿐만 아니라 설비 이용률이 높아 수익성이 높다.부유식 해상풍력발전 글로벌 1위 기업인 에퀴노르가 한국 시장에 큰 관심을 갖고 투자한 이유다. 에퀴노르는 2017년부터 세계 최초의 상용 부유식 풍력발전소인 하이윈드(Hywind) 스코틀랜드 해상풍력발전단지(30MW)를 운영 중이다. 내년부턴 노르웨이에서 세계 최대 규모 부유식 풍력발전소인 하이윈드 탐펜(88MW)을 운영한다. 미셸 지사장은 \"에퀴노르는 40년 이상의 해양 (석유·가스) 프로젝트 개발·운영 경험이 있고, 그간 진행했던 부유식 해상풍력발전 프로젝트를 통해 투자비도 굉장히 많이 낮췄다\"며 \"적은 투자비로 부유식 해상풍력 설치가 가능하고 설계기술과 O&M(운영·유지·보수) 능력 등 전체를 아우르는 밸류체인을 갖췄다\"고 자신했다. 이어 \"북해에서 허리케인 등 열악한 해상 조건에서도 해상풍력발전단지를 운영한 경험이 있다\"며 \"지금 프로젝트를 진행하는 울산 바다도 악천후로 유명한데 한국의 환경에 적합한 하부 구조 설계를 할 때 큰 도움이 될 것\"이라고 말했다. 부유식 해상풍력발전은 풍력 터빈이 바다에 떠 있고 케이블이나 체인만 해저에 계류된 방식이라 어종 등 환경에 미치는 영향도 적다. 해상풍력발전 특성상 바다를 공유하는 지역 어민들과 마찰은 피할 수 없지만, 에퀴노르가 맡은 울산 사업은 주민들을 설득하며 착실하게 진행 중이다. 에퀴노르는 지난달 울산 어민단체인 해상풍력사업어민대책위원회와 반딧불 부유식 해상풍력발전단지 상생협약을 체결한 데 이어 동해1 부유식 해상풍력발전 주민동의서를 전달받았다. 울산 반딧불, 동해1 프로젝트는 현재 풍황 계측을 끝내고 발전사업허가신청(EBL)을 준비 중이다. 2026년 이후 상업운전을 개시한다. 에퀴노르는 아울러 남해안에서 2GW(기가와트) 규모 해상풍력발전단지를 개발할 계획이다. 미셸 지사장은 \"현재 남해안에서 풍황 계측을 준비 중인 부지가 있다\"며 \"부유식과 고정식 해상풍력발전을 병행하는 방식으로 구상하고 있다\"고 밝혔다. 에퀴노르는 해상풍력발전 외에도 태양광과 수소, CCS(탄소포집·저장) 등 다양한 신재생에너지 관련 사업을 진행하고 있다. 2030년까지 회사에서 생산하는 에너지 중 16GW를 신재생에너지로 채우겠다는 목표다. 한국에서도 해상풍력발전 사업이 안정적으로 궤도에 오르면 CCS와 수소사업까지 확장할 방침이다. 미셸 지사장은 \"한국에선 해상풍력발전을 1순위로 하되 기회가 되면 다른 신재생에너지 사업도 확장할 것\"이라며 \"아직 구체적 사업 계획은 없지만 수소 관련 회의에 참석하는 등 한국 신재생에너지 상황을 예의주시하고 있다\"고 말했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=101&oid=008&aid=0004640936',\n",
              "   'title': '해상풍력발전 글로벌 최강자 韓 찾아온 5가지 이유는?'},\n",
              "  {'image_url': 'https://imgnews.pstatic.net/image/018/2021/09/05/0005029744_001_20210905153807329.jpg?type=w647',\n",
              "   'news_contents': '[이데일리 김미영 기자] 이르면 이번 주 중으로 빗썸과 코인원, 코빗 등 가상자산(암호화폐) 거래소의 은행 실명계좌 연장 여부가 결정될 전망이다. 이들 거래소는 케이뱅크와 제휴를 맺고 있는 업비트와 함께 국내 4대 거래소로 꼽히는 곳이다. 다만 업비트와는 달리 거래소 신고가 의무화되는 9월24일 이후 은행 실명계좌 발급 재계약 여부가 아직 불투명한 상황이다. 5일 금융권에 따르면 NH농협은행은 빗썸·코인원, 신한은행은 코빗과의 실명확인 계좌 발급 재계약 여부를 이르면 오는 8일 각각 발표할 예정이다. 두 은행은 지난달 말에 가상자산 거래소 3곳에 대해 현장 실사를 포함한 위험평가를 끝낸 것으로 전해졌다. 신한은행 관계자는 “가상화폐 거래소 재계약 문제는 자금세탁 관련 부서까지 모두 합의와 결재가 필요한 사안”이라며 “실사는 지난달 말에 마쳤지만, 현재는 막바지 최종 검토를 하고 있다”고 말했다. 두 은행 모두 계약 연장 여부를 미리 밝히진 않았지만, 거래소들의 금융정보분석원(FIU) 신고 마감기한인 24일이 임박해 재계약 연장을 알릴 가능성이 높게 점쳐진다. 은행과 거래소의 막판 협의에서는 거래소들의 자금세탁 방지 장치 강화가 관건이 될 것이란 관측이 많다. 특히 국제자금세탁방지기구(FATF)가 가상자산 사업자에게 부과한 ‘트래블 룰’ 의무가 아직 국내 사업자들 사이에서는 마련되지 않은 상태여서 은행들이 거래소에 이를 보완할 방안을 요구하고 있다. 트래블 룰이란 가상자산을 한 거래소에서 다른 거래소로 옮길 때 송신을 담당하는 거래소가 자산을 수신하는 거래소에 보내는 사람과 받는 사람의 정보를 제공토록 한 규정이다. 우리나라의 경우 지난 3월 시행된 개정 특정금융정보법에 이러한 트래블 룰 규정을 담았지만 업계의 정보 공유시스템 구축에 시간이 소요돼 실제 적용은 내년 3월 25일까지 1년간 유예돼 있다. 가상자산 거래소 시장의 80%가량을 차지하는 업비트는 이미 지난달 케이뱅크와 실명확인 계좌 발급 계약을 연장키로 했다. 지난달 20일에 FIU 신고를 마쳐 현재 FIU에서 업비트의 영업 허용 여부를 심사 중이다. 추가 신고 거래소가 나오지 않는다면 업비트의 독주가 이어지면서 독과점 부작용이 커질 수 있단 점을 당국도 심사과정에서 고려하리란 전망이다. 이들 대형 거래소를 제외한 중소형 거래소의 경우 은행 실명계좌를 확보하지 못한 채 ISMS(정보보호관리체계) 인증을 확보해 우선 코인간 거래를 하면서 영업을 지속해나갈 공산이 크다. 추석 연휴를 빼면 실질적으로 FIU 신고 마감일이 19일까지로 불과 2주밖에 남지 않았는데, 이 사이에 깐깐한 은행들이 실명계좌를 내어줄 가능성이 별로 없어서다. ISMS는 거래소 신고의 최소요건으로, ISMS 인증만 갖춘 거래소에선 원화 입출금이 불가능하다. 한편 금융당국은 지난달 25일 가상자산거래소 63곳 현황을 파악, 7월 말 현재 가상자산사업자 신고에 필수인 ISMS 인증을 받은 업체가 21곳이라고 발표했다. 나머지 42곳 중에선 18곳이 ISMS 인증 신청 중이었고, 24곳은 ISMS 인증 신청조차 하지 않은 것으로 확인했다. 금융당국은 ISMS 미신청 암호화폐 사업자의 폐업, 영업중단 등에 따른 피해를 우려해 투자자들에 각별한 주의를 당부하고 있다. 금융위원회 관계자는 “사전에 예치금·가상자산을 인출하는 등 선제적인 조치를 취해달라”며 “예치금·가상자산의 인출 요청을 거부·지연하거나, 갑작스러운 영업중단 등의 사례가 생기면 당국에 신고해달라”고 말했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=101&oid=018&aid=0005029744',\n",
              "   'title': '역시 대마불사?…빗썸·코인원·코빗, 실명계좌 확보 가능성'}],\n",
              " [{'image_url': 'https://imgnews.pstatic.net/image/001/2021/09/05/AKR20210905039900053_02_i_P4_20210905154812012.jpg?type=w647',\n",
              "   'news_contents': '(포항=연합뉴스) 손대성 기자 = 장기간 공사 중단으로 흉물로 방치된 경북 포항 아파트단지가 철거에 들어갔다. 5일 포항시에 따르면 북구 용흥동 금광포란재 아파트 부지 소유권자인 솔빛주택건설 등은 지난 3일 철거를 시작했다. 철거 공사 착공식에는 이강덕 시장과 김정재 국회의원을 비롯해 도의원과 시의원 등이 참석했다. 금광포란재 아파트는 지난 1997년 지하 4층, 지상 15층, 314가구 규모로 주택건설 사업계획 승인이 났다. 이후 여러 차례 사업자 변경을 거쳐 공정률 40% 상태에서 사업 주체가 부도나면서 공사가 중단됐다. 이런 상황에서 토지가 제3자에게 경매 처분돼 권리관계 분쟁이 발생했고 수년간 소송을 거치면서 도심 속 흉물로 방치되어 왔다. 시는 최근 법원 판결을 바탕으로 지난 5월 3일 토지소유자인 솔빛주택 신청에 따라 사업승인을 취소했다. 이강덕 시장은 \"시의 적극적인 노력으로 숙원사업이 해결돼 기쁘고 새로운 명품 아파트 건설로 용흥동이 발전하는 계기가 되기를 바란다\"고 말했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=102&oid=001&aid=0012643238',\n",
              "   'title': '20여년째 공사 중단 흉물 포항 도심 건물 철거…새아파트 건립'},\n",
              "  {'image_url': 'https://imgnews.pstatic.net/image/469/2021/09/05/0000627844_001_20210905154624008.png?type=w647',\n",
              "   'news_contents': \"소득 하위 88%가 1인당 25만 원씩 받는 '코로나 상생 국민지원금(5차 재난지원금)' 접수가 6일부터 시작된다. 가구별로 세대주가 신청했던 지난해와 달리  성인은 개인별로 각각 신청할 수 있으며, 신청 첫 주에는 출생연도 끝자리에 따라 요일별로 순차적으로 접수가 진행된다. 행정안전부에 따르면, 6일 오전 9시부터 온라인에서 국민지원금 지급 대상 여부 조회 및 지급 신청이 가능하다.  오프라인은 이달 13일부터 카드사와 연계된 은행이나 지자체 관할 주민센터에서 조회 및 신청을 할 수 있다. 지난해와 달리 올해는 모든 성인이 개별적으로 신청해야 하며, 미성년자는 세대주가 신청해야 한다. 대상자는 신용·체크카드 충전, 지역사랑상품권, 선불카드 중 선택해 신청할 수 있다. 신용·체크카드 충전 방식을 택할 경우엔 카드사 홈페이지·애플리케이션(앱)과 콜센터 등을 통해서, 지역사랑상품권을 택할 경우엔 주소지 관할 지자체의 지역사랑상품권 홈페이지나 앱에서 신청하면 된다. 혼잡을 막기 위해 온라인 신청 첫 주 평일에만 출생연도 끝자리에 따라 5부제를 적용한다. 9월 6일은 출생연도 끝자리가 1·6인 사람만 신청할 수 있다. 출생연도 끝자리가 2·7인 경우엔 7일, 3·8은 8일, 4·9는 9일, 5·0은 10일에 신청하면 된다. 신용·체크카드, 모바일 서울사랑상품권은 신청 다음 날부터 사용 가능하며, 선불카드는 발급 즉시 사용가능하다. 사용처는 전통시장, 식당, 미용실, 약국, 병원, 프랜차이즈 가맹점 등 지역사랑상품권을 사용할 수 있는 업종으로 국한된다. 쿠팡과 같은 대형 온라인몰, 배달의 민족과 같은 대형 배달앱, 백화점, 대형마트, 유흥업종, 프랜차이즈 직영점, 홈쇼핑 등에선 사용할 수 없다. 구체적인 사용처 정보는 별도 홈페이지(국민지원금사용처.kr)이나 네이버지도, 카카오맵 등에서 확인할 수 있다. 지원금 신청 기한은 오는 10월 29일까지이며, 사용 기한은 12월 31일까지다. 기간 내 신청하지 않거나 사용하지 않은 잔액은 모두 환수된다. \",\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=102&oid=469&aid=0000627844',\n",
              "   'title': \"'1인당 25만 원' 5차 재난지원금, 6일부터 접수 시작\"},\n",
              "  {'image_url': 'https://imgnews.pstatic.net/image/081/2021/09/05/0003213961_001_20210905154608990.jpg?type=w647',\n",
              "   'news_contents': '정부가 코로나19 백신 접종 속도를 높이기 위해 접종 완료자에 대한 추가 혜택을 검토하고 있다. 백신 접종자는 사적 모임 인원에서 제외되는 혜택을 주지만, 이보다 더 큰 혜택을 줘 18~49세 청장년층의 접종률을 대폭 늘리겠다는 복안으로 보인다. 이날 0시 기준으로 18∼49세 청장년층의 예약률은 72.2%로, 전체 대상자 1천 408만 5068명 가운데 1016만 9137명이 예약을 마쳤다. 전해철 중앙재난안전대책본부(중대본) 2차장은 5일 중대본 회의 모두발언에서 “1차 접종을 완료한 국민은 오늘 오전 3000만명(58.4%)을 넘어섰다”며 “추석 전 3600만명에 대한 1차 접종은 물론 10월 중 전 국민의 70%에 대한 접종이 완료되도록 계획에 따른 백신 도입 등에 최선을 다하겠다”고 밝혔다. 전 2차장은 또 “접종률 확대에 따른 방역수칙 일부 완화 등 접종 완료자에 대한 추가 혜택 확대도 검토해 나가겠다”며 “특별히 정부는 백신 접종 속도를 보다 높일 수 있도록 백신 도입 시점부터 전국에 소재한 접종 현장까지의 배송기간을 최소화하는 방안을 강구하고 있다”고 설명했다. 정부는 백신 수송차량을 40대 추가로 투입하고, 토요일 배송 시간을 기존 오후 3시에서 6시로 3시간 연장하기로 했다. 이를 통해 백신 도입 후 3일 이내에 현장 배송이 완료될 수 있도록 할 방침이다. 전 2차장은 코로나19 유행 상황에 대해서는 “오늘 확진자 수는 일요일 기준으로는 지난 8월 8일 이후 가장 낮은 수치(1490명)를 나타냈다”며 “최근 일주일간 국내 발생 일평균 확진자 수는 1671명으로, 8월 둘째 주 이후 3주째 지속 감소세를 보이고 있다”고 분석했다. 그는 또 “감염 재생산지수 역시 3주 연속 감소하고 있는 가운데 2주 연속 1 이하의 수치(0.98)를 나타내고 있다”고 덧붙였다. 그러면서 “내일부터 방역수칙 일부 완화가 포함된 현행 거리두기(수도권 4단계, 비수도권 3단계)가 4주 연장돼 시행된다”며 철저한 방역수칙 준수를 당부했다. 전 2차장은 “내일부터는 거리두기 3단계가 적용되고 있는 지역의 초·중·고교에서 전면등교가 시행된다”며 “안정적 등교와 차질 없는 학사 운영이 이뤄지도록 학교방역 관리에 만전을 기하겠다”고 강조했다. ',\n",
              "   'news_url': 'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=102&oid=081&aid=0003213961',\n",
              "   'title': '1차 접종 3000만명 넘자…“추가 혜택” 백신 속도전 검토'}])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTO8WqMW-9dS"
      },
      "source": [
        "###################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgwDf8G_ANZ"
      },
      "source": [
        "# 위의 코드 테스트 해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvWf1j4dlBxm",
        "outputId": "8febd4cf-1546-4fd6-b7b2-17f433284248"
      },
      "source": [
        "# 크롤링 테스트\n",
        "url = 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=100&oid=014&aid=0004699768'\n",
        "headers = {'User-Agent' : 'Mozilla/5.0'}\n",
        "res = requests.get(url, headers = headers)\n",
        "soup = BeautifulSoup(res.text,'html.parser')\n",
        "body = soup.select(\"div._article_body_contents\")[0]\n",
        "body"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<div class=\"_article_body_contents\" id=\"articleBodyContents\">\n",
              "<!-- 본문 내용 -->\n",
              "<!-- TV플레이어 -->\n",
              "<!-- // TV플레이어 -->\n",
              "<script type=\"text/javascript\">\n",
              "// flash 오류를 우회하기 위한 함수 추가\n",
              "function _flash_removeCallback() {}\n",
              "</script>\n",
              "<strong class=\"media_end_summary\">박수현 수석 '브리핑에 없는 대통령 이야기' <br/>\"CVC 규제 완화, 대기업-벤처 동반성장 촉진\"</strong> <span class=\"end_photo_org\"><img alt=\"\" src=\"https://imgnews.pstatic.net/image/014/2021/08/29/0004699768_001_20210829234204160.jpg?type=w647\"><em class=\"img_desc\">[서울=뉴시스] 김진아 기자 = 문재인 대통령이 26일 청와대에서 열린 제2벤처붐 성과보고회 ‘K+벤처’에서 발언을 하고 있다. 2021.08.26. bluesoda@newsis.com /사진=뉴시스</em></img></span> <br/><br/>[파이낸셜뉴스] 청와대는 29일 문재인 정부의 기업형 벤처캐피탈(CVC) 관련 규제 완화 정책이 향후 대기업과 벤처기업의 동반성장을 촉진할 것이라고 기대했다.  <br/> <br/>박수현 청와대 국민소통수석은 이날 자신의 페이스북에 올린 '브리핑에 없는 대통령 이야기-제2 벤처붐을 일으킨 정부의 5대 지원정책'이라는 제목의 글을 통해 이같이 밝혔다.  <br/> <br/>박 수석은 \"문재인 정부는 금산분리 원칙 완화에 따른 부작용은 최소화하되, 벤처 활성화를 유도하기 위해 일반지주회사의 CVC 제한적 보유 허용을 추진했다\"고 말했다. 이 정책은 총수 일가가 지분을 가진 기업에 투자하지 않는 것 등을 전제로 대기업 지주회사의 CVC 완전자회사 보유를 허용한 것이다. 2020년 '하반기 경제정책 방향'에서 발표했고 지난해 7월 30일 '비상경제 중대본' 회의에서 의결했다.  <br/> <br/>박 수석은 \"이 정책은 대기업의 신속하고 적극적인 투자를 활성화할 것\"이라며 \"벤처생태계를 질적으로 향상시킬 것\"이라고 강조했다  <br/>박 수석은 이와 함께 △중소벤처기업부 출범 △모태펀드 예산 대폭 확대 △유니콘기업 육성을 위한 'K 유니콘 프로젝트' △벤처투자촉진에 관한 법률 제정 등을 문재인 정부의 5대 지원 정책으로 소개했다.  <br/> <br/>박 수석은 \"무엇보다 벤처기업인 스스로의 '창의적인 아이디어와 도전 정신'이 1등 공신임은 아무리 강조해도 지나침이 없다\"며 \"다만, 정부와 사회가 벤처기업이 도전하고 성과를 낼 수 있도록 제도적·환경적으로 어떻게, 무엇을 뒷받침해 왔는가를 짚어보고, 그것이 앞으로의 과제 해결에 던지는 의미와 시사점을 정리해 보고자 하는 것\"이라고 글을 쓴 배경을 설명했다.  <br/> <br/>박 수석은 특히 \"아마 대통령께서 재임 중에 지금까지 각종 회의에서 말씀하신 '벤처'라는 단어만 세어봐도 수백 번은 될 것이다\"라는 청와대 한 참모의 말을 전하며 문 대통령의 벤처육성 의지를 강조하기도 했다.\n",
              "\t<!-- // 본문 내용 -->\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GbcEe4pZlBpN",
        "outputId": "1fcb53b5-6f90-488c-d00e-f5d155c43753"
      },
      "source": [
        "# 크롤링 테스트\n",
        "url = 'https://news.naver.com/main/home.naver'\n",
        "headers = {'User-Agent' : 'Mozilla/5.0'}\n",
        "res = requests.get(url, headers = headers)\n",
        "soup = BeautifulSoup(res.text,'html.parser')\n",
        "# body = soup.select(\"div._article_body_contents\")[0]\n",
        "sec = 'politics'\n",
        "s = soup.find(\"div\", attrs={'class':'main_component droppable', 'id':'section_'+sec}).find_all('li', limit=3)\n",
        "s[0].a.attrs.get('href') # url 구하기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=100&oid=016&aid=0001883465'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mex3MhJplBkX"
      },
      "source": [
        "###################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBfmnTuyr0rW"
      },
      "source": [
        "# 학습한 모델 가져와서 요약하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0kGjInGr0k5",
        "outputId": "48357437-1607-48dd-ee67-371908b10466"
      },
      "source": [
        "!pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cpu\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-1.8.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (169.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 169.1 MB 28 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cpu\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.9.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (13.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.3 MB 136 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cpu) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cpu) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cpu) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1+cpu torchaudio-0.8.1 torchvision-0.9.1+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFHh4nxYr0i4",
        "outputId": "9837add0-c650-4af9-f3f9-0d3d73d78ee4"
      },
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrouge\n",
            "  Downloading pyrouge-0.1.3.tar.gz (60 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▍                          | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 60 kB 4.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-py3-none-any.whl size=191620 sha256=d19ca9aefe872314449a47cd930505d35b08bfdaff54f9519733787c9817bc9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/35/6a/ffb9a1f51b2b00fee42e7f67f5a5d8e10c67d048cda09ccd57\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n",
            "Collecting https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "  Downloading https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "\u001b[K     \\ 202 kB 337 kB/s\n",
            "\u001b[?25hRequirement already satisfied: pyrouge in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Name: pyrouge\n",
            "Version: 0.1.3\n",
            "Summary: A Python wrapper for the ROUGE summarization evaluation package.\n",
            "Home-page: https://github.com/noutenki/pyrouge\n",
            "Author: Benjamin Heinzerling, Anders Johannsen\n",
            "Author-email: benjamin.heinzerling@h-its.org\n",
            "License: LICENSE.txt\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "Cloning into 'pyrouge'...\n",
            "remote: Enumerating objects: 393, done.\u001b[K\n",
            "remote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
            "Receiving objects: 100% (393/393), 298.74 KiB | 1.19 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "2021-09-05 06:49:23,709 [MainThread  ] [INFO ]  Set ROUGE home directory to pyrouge/tools/ROUGE-1.5.5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxMvvQUur0hI",
        "outputId": "a87ae44a-2d89-4ffb-eba7-0c822ea8d1ed"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "!pip install easydict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvnHuAK9r0fR",
        "outputId": "1030b330-17b7-4825-8617-3f73116de1b7"
      },
      "source": [
        "!git clone https://github.com/HaloKim/KorBertSum.git\n",
        "%cd /content/KorBertSum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KorBertSum'...\n",
            "remote: Enumerating objects: 11142, done.\u001b[K\n",
            "remote: Counting objects: 100% (10840/10840), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7610/7610), done.\u001b[K\n",
            "remote: Total 11142 (delta 3259), reused 10780 (delta 3220), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (11142/11142), 18.97 MiB | 14.24 MiB/s, done.\n",
            "Resolving deltas: 100% (3434/3434), done.\n",
            "/content/KorBertSum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l_xupW_r0dg"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/KorBertSum/src')\n",
        "\n",
        "# import sys\n",
        "# sys.path.append(\"..\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnjBjsBpr0b1"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from others.logging import logger, init_logger\n",
        "from transformers import BertConfig, BertTokenizer\n",
        "from tensorboardX import SummaryWriter\n",
        "import easydict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoEcfFh1r0Zo",
        "outputId": "2fcc5568-402b-4c8a-fb87-8f391b16290c"
      },
      "source": [
        "%cd '/content/KorBertSum/'\n",
        "\n",
        "import os\n",
        "\n",
        "os.chdir('/content/KorBertSum/src')\n",
        "\n",
        "from models import data_loader, model_builder\n",
        "from models.model_builder import Summarizer\n",
        "from models.data_loader import load_dataset\n",
        "from models.reporter import ReportMgr\n",
        "from models.stats import Statistics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KorBertSum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTzPjHqWr0WT"
      },
      "source": [
        "def _tally_parameters(model):\n",
        "    n_params = sum([p.nelement() for p in model.parameters()])\n",
        "    return n_params\n",
        "\n",
        "def build_trainer(args, device_id, model,\n",
        "                  optim):\n",
        "    \"\"\"\n",
        "    Simplify `Trainer` creation based on user `opt`s*\n",
        "    Args:\n",
        "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
        "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
        "        fields (dict): dict of fields\n",
        "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
        "        data_type (str): string describing the type of data\n",
        "            e.g. \"text\", \"img\", \"audio\"\n",
        "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
        "            used to save the model\n",
        "    \"\"\"\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "\n",
        "\n",
        "    grad_accum_count = args.accum_count\n",
        "    n_gpu = args.world_size\n",
        "\n",
        "    if device_id >= 0:\n",
        "        gpu_rank = int(args.gpu_ranks[device_id])\n",
        "    else:\n",
        "        gpu_rank = 0\n",
        "        n_gpu = 0\n",
        "\n",
        "    print('gpu_rank %d' % gpu_rank)\n",
        "\n",
        "    tensorboard_log_dir = args.model_path\n",
        "\n",
        "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
        "\n",
        "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
        "\n",
        "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
        "\n",
        "    # print(tr)\n",
        "    if (model):\n",
        "        n_params = _tally_parameters(model)\n",
        "        logger.info('* number of parameters: %d' % n_params)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Class that controls the training process.\n",
        "\n",
        "    Args:\n",
        "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
        "                to train\n",
        "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
        "               the optimizer responsible for update\n",
        "            trunc_size(int): length of truncated back propagation through time\n",
        "            shard_size(int): compute loss in shards of this size for efficiency\n",
        "            data_type(string): type of the source input: [text|img|audio]\n",
        "            norm_method(string): normalization methods: [sents|tokens]\n",
        "            grad_accum_count(int): accumulate gradients this many times.\n",
        "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
        "                the object that creates reports, or None\n",
        "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
        "                used to save a checkpoint.\n",
        "                Thus nothing will be saved if this parameter is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  args, model,  optim,\n",
        "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
        "                  report_manager=None):\n",
        "        # Basic attributes.\n",
        "        self.args = args\n",
        "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.grad_accum_count = grad_accum_count\n",
        "        self.n_gpu = n_gpu\n",
        "        self.gpu_rank = gpu_rank\n",
        "        self.report_manager = report_manager\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(reduction='none')\n",
        "        assert grad_accum_count > 0\n",
        "        # Set model in training mode.\n",
        "        if (model):\n",
        "            self.model.train()\n",
        "\n",
        "    def summ(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
        "      \"\"\" Validate model.\n",
        "          valid_iter: validate data iterator\n",
        "      Returns:\n",
        "          :obj:`nmt.Statistics`: validation loss statistics\n",
        "      \"\"\"\n",
        "      # Set model in validating mode.\n",
        "      def _get_ngrams(n, text):\n",
        "          ngram_set = set()\n",
        "          text_length = len(text)\n",
        "          max_index_ngram_start = text_length - n\n",
        "          for i in range(max_index_ngram_start + 1):\n",
        "              ngram_set.add(tuple(text[i:i + n]))\n",
        "          return ngram_set\n",
        "\n",
        "      def _block_tri(c, p):\n",
        "          tri_c = _get_ngrams(3, c.split())\n",
        "          for s in p:\n",
        "              tri_s = _get_ngrams(3, s.split())\n",
        "              if len(tri_c.intersection(tri_s))>0:\n",
        "                  return True\n",
        "          return False\n",
        "\n",
        "      if (not cal_lead and not cal_oracle):\n",
        "          self.model.eval()\n",
        "      stats = Statistics()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for batch in test_iter:\n",
        "              src = batch.src\n",
        "              labels = batch.labels\n",
        "              segs = batch.segs\n",
        "              clss = batch.clss\n",
        "              mask = batch.mask\n",
        "              mask_cls = batch.mask_cls\n",
        "\n",
        "              if (cal_lead):\n",
        "                  selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "              elif (cal_oracle):\n",
        "                  selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                  range(batch.batch_size)]\n",
        "              else:\n",
        "                  sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "                  sent_scores = sent_scores + mask.float()\n",
        "                  sent_scores = sent_scores.cpu().data.numpy()\n",
        "                  selected_ids = np.argsort(-sent_scores, 1)\n",
        "      return selected_ids\n",
        "\n",
        "\n",
        "\n",
        "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
        "                               report_stats):\n",
        "        if self.grad_accum_count > 1:\n",
        "            self.model.zero_grad()\n",
        "\n",
        "        for batch in true_batchs:\n",
        "            if self.grad_accum_count == 1:\n",
        "                self.model.zero_grad()\n",
        "\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "\n",
        "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "            loss = self.loss(sent_scores, labels.float())\n",
        "            loss = (loss*mask.float()).sum()\n",
        "            (loss/loss.numel()).backward()\n",
        "            # loss.div(float(normalization)).backward()\n",
        "\n",
        "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
        "\n",
        "\n",
        "            total_stats.update(batch_stats)\n",
        "            report_stats.update(batch_stats)\n",
        "\n",
        "            # 4. Update the parameters and statistics.\n",
        "            if self.grad_accum_count == 1:\n",
        "                # Multi GPU gradient gather\n",
        "                if self.n_gpu > 1:\n",
        "                    grads = [p.grad.data for p in self.model.parameters()\n",
        "                             if p.requires_grad\n",
        "                             and p.grad is not None]\n",
        "                    distributed.all_reduce_and_rescale_tensors(\n",
        "                        grads, float(1))\n",
        "                self.optim.step()\n",
        "\n",
        "        # in case of multi step gradient accumulation,\n",
        "        # update only after accum batches\n",
        "        if self.grad_accum_count > 1:\n",
        "            if self.n_gpu > 1:\n",
        "                grads = [p.grad.data for p in self.model.parameters()\n",
        "                         if p.requires_grad\n",
        "                         and p.grad is not None]\n",
        "                distributed.all_reduce_and_rescale_tensors(\n",
        "                    grads, float(1))\n",
        "            self.optim.step()\n",
        "\n",
        "    def _save(self, step):\n",
        "        real_model = self.model\n",
        "        # real_generator = (self.generator.module\n",
        "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
        "        #                   else self.generator)\n",
        "\n",
        "        model_state_dict = real_model.state_dict()\n",
        "        # generator_state_dict = real_generator.state_dict()\n",
        "        checkpoint = {\n",
        "            'model': model_state_dict,\n",
        "            # 'generator': generator_state_dict,\n",
        "            'opt': self.args,\n",
        "            'optim': self.optim,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
        "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
        "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
        "        if (not os.path.exists(checkpoint_path)):\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            return checkpoint, checkpoint_path\n",
        "\n",
        "    def _start_report_manager(self, start_time=None):\n",
        "        \"\"\"\n",
        "        Simple function to start report manager (if any)\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            if start_time is None:\n",
        "                self.report_manager.start()\n",
        "            else:\n",
        "                self.report_manager.start_time = start_time\n",
        "\n",
        "    def _maybe_gather_stats(self, stat):\n",
        "        \"\"\"\n",
        "        Gather statistics in multi-processes cases\n",
        "\n",
        "        Args:\n",
        "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
        "                or None (it returns None in this case)\n",
        "\n",
        "        Returns:\n",
        "            stat: the updated (or unchanged) stat object\n",
        "        \"\"\"\n",
        "        if stat is not None and self.n_gpu > 1:\n",
        "            return Statistics.all_gather_stats(stat)\n",
        "        return stat\n",
        "\n",
        "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
        "                               report_stats):\n",
        "        \"\"\"\n",
        "        Simple function to report training stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_training(\n",
        "                step, num_steps, learning_rate, report_stats,\n",
        "                multigpu=self.n_gpu > 1)\n",
        "\n",
        "    def _report_step(self, learning_rate, step, train_stats=None,\n",
        "                     valid_stats=None):\n",
        "        \"\"\"\n",
        "        Simple function to report stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_step(\n",
        "                learning_rate, step, train_stats=train_stats,\n",
        "                valid_stats=valid_stats)\n",
        "\n",
        "    def _maybe_save(self, step):\n",
        "        \"\"\"\n",
        "        Save the model if a model saver is set\n",
        "        \"\"\"\n",
        "        if self.model_saver is not None:\n",
        "            self.model_saver.maybe_save(step)\n",
        "\n",
        "class BertData():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
        "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
        "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "    def preprocess(self, src):\n",
        "\n",
        "        if (len(src) == 0):\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [' '.join(s) for s in src]\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
        "\n",
        "        src = [src[i][:2000] for i in idxs]\n",
        "        src = src[:1000]\n",
        "\n",
        "        if (len(src) < 3):\n",
        "            return None\n",
        "\n",
        "        src_txt = [' '.join(sent) for sent in src]\n",
        "        text = ' [SEP] [CLS] '.join(src_txt)\n",
        "        src_subtokens = self.tokenizer.tokenize(text)\n",
        "        src_subtokens = src_subtokens[:510]\n",
        "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
        "\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        labels = None\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "        tgt_txt = None\n",
        "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
        "\n",
        "def _lazy_dataset_loader(pt_file):\n",
        "  yield  pt_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnq3Ifxtr0KC"
      },
      "source": [
        "args = easydict.EasyDict({\n",
        "    \"encoder\":'classifier',\n",
        "    \"mode\":'test',\n",
        "    \"bert_data_path\":'/content/drive/MyDrive/BERT_ex/bert_data/korean',\n",
        "    \"model_path\":'/content/drive/MyDrive/BERT_ex/bert_data/bert_classifier',\n",
        "    \"result_path\":'./results',\n",
        "    \"temp_dir\":'./temp',\n",
        "    \"batch_size\":1000,\n",
        "    \"use_interval\":True,\n",
        "    \"hidden_size\":128,\n",
        "    \"ff_size\":512,\n",
        "    \"heads\":4,\n",
        "    \"inter_layers\":2,\n",
        "    \"rnn_size\":512,\n",
        "    \"param_init\":0,\n",
        "    \"param_init_glorot\":True,\n",
        "    \"dropout\":0.1,\n",
        "    \"optim\":'adam',\n",
        "    \"lr\":2e-3,\n",
        "    \"report_every\":1,\n",
        "    \"save_checkpoint_steps\":5,\n",
        "    \"block_trigram\":True,\n",
        "    \"recall_eval\":False,\n",
        "    \n",
        "    \"accum_count\":1,\n",
        "    \"world_size\":1,\n",
        "    \"visible_gpus\":'-1',\n",
        "    \"gpu_ranks\":'0',\n",
        "    \"log_file\":'/content/drive/MyDrive/BERT_ex/bert_data/logs/log.log',\n",
        "    \"test_from\":'/content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt'\n",
        "})\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2SGU-05rz_9"
      },
      "source": [
        "def test(args, input_list, device_id, pt, step):\n",
        "  init_logger(args.log_file)\n",
        "  device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "  device_id = 0 if device == \"cuda\" else -1\n",
        "\n",
        "  cp = args.test_from\n",
        "  try:\n",
        "    step = int(cp.split('.')[-2].split('_')[-1])\n",
        "  except:\n",
        "    step = 0\n",
        "\n",
        "  device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "  if (pt != ''):\n",
        "      test_from = pt\n",
        "  else:\n",
        "      test_from = args.test_from\n",
        "  logger.info('Loading checkpoint from %s' % test_from)\n",
        "  checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
        "  opt = vars(checkpoint['opt'])\n",
        "  for k in opt.keys():\n",
        "      if (k in model_flags):\n",
        "        setattr(args, k, opt[k])\n",
        "\n",
        "  config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
        "  model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "  model.load_cp(checkpoint)\n",
        "  model.eval()\n",
        "\n",
        "  test_iter = data_loader.Dataloader(args, _lazy_dataset_loader(input_list),\n",
        "                                args.batch_size, device,\n",
        "                                shuffle=False, is_test=True)\n",
        "  trainer = build_trainer(args, device_id, model, None)\n",
        "  result = trainer.summ(test_iter,step)\n",
        "  return result, input_list\n",
        "\n",
        "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aKgoqWrtEq6"
      },
      "source": [
        "def txt2input(text):\n",
        "  data = list(filter(None, text.split('.')))\n",
        "  bertdata = BertData()\n",
        "  txt_data = bertdata.preprocess(data)\n",
        "  data_dict = {\"src\":txt_data[0],\n",
        "               \"labels\":[0,1,2],\n",
        "               \"segs\":txt_data[2],\n",
        "               \"clss\":txt_data[3],\n",
        "               \"src_txt\":txt_data[4],\n",
        "               \"tgt_txt\":None}\n",
        "  input_data = []\n",
        "  input_data.append(data_dict)\n",
        "  return input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j6VxY_etAMr"
      },
      "source": [
        "# input_data = txt2input(text)\n",
        "# sum_list = test(args, input_data, -1, '', None)\n",
        "# sum_list[0]\n",
        "# [list(filter(None, text.split('\\n')))[i] for i in sum_list[0][0][:3]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNKrPlSiydDc"
      },
      "source": [
        "# input_data = txt2input(news_info['news_contents'])\n",
        "# sum_list = test(args, input_data, -1, '', None)\n",
        "# sum_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "IVqh9VRByFcW",
        "outputId": "0db860ac-b1a0-4566-d743-7e6bedf2ed65"
      },
      "source": [
        "news_info['news_contents']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[헤럴드경제=정윤희 기자]이준석 국민의힘 대표는 5일 “정홍원 당 선거관리위원장이 많은 고생을 하고 계시기 때문에 더 큰 성원과 지지, 신뢰를 보낸다는 말씀을 드린다”고 정 위원장에 힘을 실었다. 경선룰을 둘러싼 갈등이 첨예해지며 일부 대선주자들이 당 공식행사에 불참하고, 정 위원장의 사의설까지 나온 것을 염두에 둔 것으로 풀이된다. 이 대표는 이날 서울 여의도 중앙당사에서 열린 대선주자 간담회 및 경선후보 공정경선 서약식에서 홍준표 의원과 유승민 전 의원 등을 겨냥해 “오늘 우리 경선에 서막을 알리는 공정선거 서약 자리에 빠진 자리들이 있는 거 같아서 당 대표로서 매우 유감”이라며 이같이 말했다. 이날 홍 의원과 유 전 의원, 하태경 의원, 안상수 전 인천시장 등 대선주자 4명은 앞서 예고한대로 공정경선 서약식에 불참했다. 이들은 역선택 방지 조항을 도입 않기로 한 경선준비위원회 안을 확정하라고 요구하고 있다. 박찬주 예비역 대장은 당초 ‘보이콧 입장문’에 이름을 올렸지만, 이날 행사에 참석했다. 이 대표는 이들을 겨냥해 “당 선거관리에 전권을 부여받은 선관위의 운영에 다소간의 불만이 있다고 해서 당 공식행사에 불참하는 행위에 대해서 매우 우려스럽고 다시 반복돼선 안 된다는 생각하게 된다”며 “앞으로 주자들 경우에 다소간 이견 있다하더라도 성숙한 방식으로 본인들의 의사를 표현하고 최소한 선관위에 대한 기본적 예의를 지켜야 된다”고 했다. 그러면서 “당 대표로서 말하지만 지난 2012년 총선을 승리로 이끄셨던 공관위원장이셨던, 우리 정부에서 존경받는 총리 역임하신 정홍원 선관위원장께선 지도부에 무한한 신임과 지지를 받고 계신다”며 “항상 우리 당에 어려울 때마다 많은 도움 주시는 정 총리께 당 대표로서 감사하다는 말씀을 드린다”고 힘을 실었다. '"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIeunYGrrz9Q",
        "outputId": "b99fdc0a-c451-4b02-b11d-e883937b7529"
      },
      "source": [
        "##########\n",
        " # 뉴스 요약하기\n",
        "\n",
        "sections = ['pol','eco','soc']\n",
        "\n",
        "for section in sections:\n",
        "\n",
        "  for news_info in news_dic[section]:\n",
        "    input_data = txt2input(news_info['news_contents'])  # 전처리\n",
        "    sum_list = test(args, input_data, -1, '', None)\n",
        "    # get_index = sum_list[0]\n",
        "    news_info['news_contents_summ'] = [list(filter(None, news_info['news_contents'].split('.')))[i] for i in sum_list[0][0][:3]]\n",
        "\n",
        "  # 요약 결과\n",
        "  print('\\n[', section,']')\n",
        "  for i in range(3):\n",
        "    print('\\nOriginal)')\n",
        "    print(news_dic[section][i]['news_contents'])\n",
        "    print('\\nSummary)')\n",
        "    print(news_dic[section][i]['news_contents_summ'])\n",
        "    print('\\n')\n",
        "\n",
        "# news_info['news_contents']에서 네이버 뉴스를 가져온뒤, 학습시킨 모델에 넣고 요약문을 얻은 뒤, \n",
        "# news_info에서 'news_contents_summ' 변수에 요약문을 새로 지정한다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:13:05,812 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:13:12,423 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:13:18,246 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:13:24,844 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:13:30,453 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:13:37,016 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n",
            "\n",
            "[ pol ]\n",
            "\n",
            "Original)\n",
            "[헤럴드경제=정윤희 기자]이준석 국민의힘 대표는 5일 “정홍원 당 선거관리위원장이 많은 고생을 하고 계시기 때문에 더 큰 성원과 지지, 신뢰를 보낸다는 말씀을 드린다”고 정 위원장에 힘을 실었다. 경선룰을 둘러싼 갈등이 첨예해지며 일부 대선주자들이 당 공식행사에 불참하고, 정 위원장의 사의설까지 나온 것을 염두에 둔 것으로 풀이된다. 이 대표는 이날 서울 여의도 중앙당사에서 열린 대선주자 간담회 및 경선후보 공정경선 서약식에서 홍준표 의원과 유승민 전 의원 등을 겨냥해 “오늘 우리 경선에 서막을 알리는 공정선거 서약 자리에 빠진 자리들이 있는 거 같아서 당 대표로서 매우 유감”이라며 이같이 말했다. 이날 홍 의원과 유 전 의원, 하태경 의원, 안상수 전 인천시장 등 대선주자 4명은 앞서 예고한대로 공정경선 서약식에 불참했다. 이들은 역선택 방지 조항을 도입 않기로 한 경선준비위원회 안을 확정하라고 요구하고 있다. 박찬주 예비역 대장은 당초 ‘보이콧 입장문’에 이름을 올렸지만, 이날 행사에 참석했다. 이 대표는 이들을 겨냥해 “당 선거관리에 전권을 부여받은 선관위의 운영에 다소간의 불만이 있다고 해서 당 공식행사에 불참하는 행위에 대해서 매우 우려스럽고 다시 반복돼선 안 된다는 생각하게 된다”며 “앞으로 주자들 경우에 다소간 이견 있다하더라도 성숙한 방식으로 본인들의 의사를 표현하고 최소한 선관위에 대한 기본적 예의를 지켜야 된다”고 했다. 그러면서 “당 대표로서 말하지만 지난 2012년 총선을 승리로 이끄셨던 공관위원장이셨던, 우리 정부에서 존경받는 총리 역임하신 정홍원 선관위원장께선 지도부에 무한한 신임과 지지를 받고 계신다”며 “항상 우리 당에 어려울 때마다 많은 도움 주시는 정 총리께 당 대표로서 감사하다는 말씀을 드린다”고 힘을 실었다. \n",
            "\n",
            "Summary)\n",
            "['[헤럴드경제=정윤희 기자]이준석 국민의힘 대표는 5일 “정홍원 당 선거관리위원장이 많은 고생을 하고 계시기 때문에 더 큰 성원과 지지, 신뢰를 보낸다는 말씀을 드린다”고 정 위원장에 힘을 실었다', ' 이날 홍 의원과 유 전 의원, 하태경 의원, 안상수 전 인천시장 등 대선주자 4명은 앞서 예고한대로 공정경선 서약식에 불참했다', ' 이 대표는 이들을 겨냥해 “당 선거관리에 전권을 부여받은 선관위의 운영에 다소간의 불만이 있다고 해서 당 공식행사에 불참하는 행위에 대해서 매우 우려스럽고 다시 반복돼선 안 된다는 생각하게 된다”며 “앞으로 주자들 경우에 다소간 이견 있다하더라도 성숙한 방식으로 본인들의 의사를 표현하고 최소한 선관위에 대한 기본적 예의를 지켜야 된다”고 했다']\n",
            "\n",
            "\n",
            "\n",
            "Original)\n",
            "정홍원 국민의힘 선거관리위원장이 '역선택 방지 문항' 논란과 관련해 \"어디에 치우친 게 아니고 민주적 의사에 따라 하려고 한다\"며 \"일방적으로 누구를 유리하게 하려고 한다는 선입견을 갖지 말라\"고 당부했다. 자신의 사의 표명 보도에 대해선 언급하지 않았다. 정 위원장은 5일 서울 여의도 국민의힘 당사에서 역선택 방지 문항 논의 절차에 대해 구체적으로 설명했다. 그는 \"후보들의 찬반 의견을 듣고 찬성 2명, 반대 2명, 중립 2명 전문가를 모시고 의견을 청취했다\"며 \"핵심 요지는 '역선택 우려는 있다, 그러나 과학적으로 증명되지 않았다'다\"고 말했다. 그러면서 \"위원들이 장시간 토론한 결과 2가지 안으로 압축됐다. 하나는 방지 문항을 두지 않는 안이고, (다른 안은) 역선택 문항이 있는 여론조사와 없는 여론조사 결과를 합산하는 두 개의 안으로 압축됐다\"며 \"이것에 대한 논의 결과는 반반이었다\"고 말했다. 정 위원장은 \"결론을 내지 못하고 주말에 시간을 갖고 결론을 내기로 한 상황이다\"고 강조했다. 이날 서약식에는 역선택 방지 문항에 반대한 홍준표 의원과 유승민 전 의원 등이 불참했다. 이준석 국민의힘 대표는 강한 유감을 표명했다. 이 대표는 \"오늘 선관위에 다소간 불만이 있다고 해서 불참한 행위에 대해선 매우 우렵스럽고 다시는 반복돼선 안 된다\"며 \"이견이 있더라도 성숙한 방식으로 본인들의 의사를 표명하고 최소한 선관위에 대한 기본적인 예의를 지켜야 한다고 말씀드린다\"고 말했다. 그러면서 \"다시 한 번 공정선거 서약식에 모든 후보들이 오지 못한 것에 유감이다\"고 지적했다. 이 대표는 정 위원장을 향해 \"지도부는 무한한 신뢰와 지지를 보내고 있다\"며 \"더 큰 성원과 지지, 신뢰를 보낸다\"고 말했다. 서약식에 앞서 정 위원장이 이 대표에게 사의를 표명했다는 보도를 의식한 발언으로 풀이된다. 장성민 세계와동북아평화포럼 이사장이 발언 도중 사의 표명 보도에 대해 묻자 이 대표를 한 손을 가로젓기도 했다. \n",
            "\n",
            "Summary)\n",
            "[' 정 위원장은 5일 서울 여의도 국민의힘 당사에서 역선택 방지 문항 논의 절차에 대해 구체적으로 설명했다', ' 그는 \"후보들의 찬반 의견을 듣고 찬성 2명, 반대 2명, 중립 2명 전문가를 모시고 의견을 청취했다\"며 \"핵심 요지는 \\'역선택 우려는 있다, 그러나 과학적으로 증명되지 않았다\\'다\"고 말했다', ' 그러면서 \"위원들이 장시간 토론한 결과 2가지 안으로 압축됐다']\n",
            "\n",
            "\n",
            "\n",
            "Original)\n",
            "여야 대선주자들이 윤석열 전 검찰총장의 '고발 사주' 의혹을 겨냥한 총공세를 펼치고 있다. 윤 전 총장은 '정치 공작'이라며 해명하고 있지만, 납득이 가지 않는다고 반응하면서다. 여당 주자들에게는 본선에서 맞붙을 가능성이 가장 큰 상대에 대한 견제 목적이, 야당 주자들에게는 야권 1위 후보 추격을 위한  동력을 확보하고 존재감을 키우려는 계산이 깔려 있다. 더불어민주당 대선주자인 이재명 경기지사는 5일 대구상공회의소에서 가진 기자간담회에서 윤 전 검찰총장에 대해 \"검찰권을 사적으로 남용하는 데 개입했다는 의혹이 지금 계속 나오고 있다\"며 \"본인이 적폐 그 자체였던 것 같다\"고 직격했다. 이어 \"진실이 아니길 바라지만, 알고도 방치했다면 민주주의 질서 자체를 위협하는 국정농단 그 자체이고 본인이 청산돼야 할 적폐 세력 자체\"라고도 했다. '추-윤 갈등'의 당사자인 추미애 전 법무부 장관도 이날 페이스북에 윤 전 총장 측이 해당 의혹에 \"증거를 대라\"고 대응한 것을 지적하며 \"일국의 검찰총장까지 지낸 분의 언사로는 대단히 부적절해 보인다\"고 비판했다. 그러면서 \"궁지에 몰린 범죄자들이 뭔가 두려운 장래를 직감하고 마지막 순간에 입에 다는 언사\"라고 했다. 추 전 장관은 \"윤석열은 이제 더이상 무소불위의 검찰총장 신분이 아니고 깨알 검증을 피할 수 없는 대권후보\"라고 했다. 윤 전 총장 캠프에서 '추미애 사단의 정치공작'이라 주장한 것에는 \"황당한 말을 난사한다\"며 일축했다. 국민의힘 내 경쟁주자인 홍준표 의원은 4, 5일 페이스북에 윤 전 총장을 비판하는 메시지를 잇따라 올렸다. 그는 4일 \"참 보기가 딱하다. 수사 공작은 간첩 잡는 대공 수사 때나 하는 것이다. '증거 내놔라' 식의 우격다짐만으로는 수습이 안 될 것 같다\"고 꼬집었다. 5일에는 \"곧 드러날 일을 공작정치 운운으로 대응하는 것은 기존 정치인들이 통상 하는 무조건 부인하고 보자는 배 째라식 후안무치 대응\"이라며 \"이제 진실게임에 들어가 버려 일이 커질 대로 커졌다\"고 지적했다. 그러면서 \"지금이라도 진실을 고백하고 대국민 사과를 하시라\"고 했다. 유승민 전 의원도  4일 페이스북에 \"윤 전 총장 의혹이 만약 사실이라면, 이는 검찰총장의 공권력을 사유화한 헌법 유린 범죄\"라고 주장했다. 특히 윤 전 총장에게 ①고발 사주 의혹 관련 서류 작성과 전달 과정을 알고 있었는지 ②관여·지시한 사실이 드러나면 후보직 사퇴 여부에 대한 입장을 국민 앞에서 밝힐 것을 촉구했다. 윤 전 총장은 지난해 4월 21대 총선에 앞서 자신의 최측근 검사가 국민의힘에 범여권 정치인에 대한 고발을 사주했다는 의혹을 받고 있다. 윤 전 총장은 사실무근이라고 주장하고 있지만, 당내에서조차 경선을 앞두고 논란이 수그러들 기미가 보이지 않고 있다. 윤 전 총장을 겨냥한 공세가 확산되고 있지만, 국민의힘 지도부는 적극적인 엄호보다는  신중한 태도를 보이고 있다. 이준석 대표는 이날  KBS인터뷰에서 \"당무 감사를 통해 살펴보겠지만, 당무 감사의 범위는 굉장히 좁다\"며 \"검찰에서 빨리 감찰을 통해 결론을 내려야 한다\"고만 말했다. \n",
            "\n",
            "Summary)\n",
            "[' 여당 주자들에게는 본선에서 맞붙을 가능성이 가장 큰 상대에 대한 견제 목적이, 야당 주자들에게는 야권 1위 후보 추격을 위한  동력을 확보하고 존재감을 키우려는 계산이 깔려 있다', ' 더불어민주당 대선주자인 이재명 경기지사는 5일 대구상공회의소에서 가진 기자간담회에서 윤 전 검찰총장에 대해 \"검찰권을 사적으로 남용하는 데 개입했다는 의혹이 지금 계속 나오고 있다\"며 \"본인이 적폐 그 자체였던 것 같다\"고 직격했다', ' 그러면서 \"궁지에 몰린 범죄자들이 뭔가 두려운 장래를 직감하고 마지막 순간에 입에 다는 언사\"라고 했다']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:13:42,546 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:13:49,132 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:13:54,686 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:14:01,274 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:14:06,859 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:14:13,402 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n",
            "\n",
            "[ eco ]\n",
            "\n",
            "Original)\n",
            "[세종=이데일리 원다연 기자] 내년 공무원·군인·국민·사학연금 등 4대 공적연금의 적자 보전 등을 위해 투입되는 국가재정이 8조7000억원에 달할 전망이다. 저출산과 고령화로 연금을 받는 수급자가 급증하면서 눈덩이처럼 적자가 불어나고 있어 미래세대를 고려한 연금 개혁이 시급하다는 지적이 나온다. 5일 기획재정부가 국회에 제출한 `2021~2025년 국가재정 운용계획`에 따르면 적자 보전과 사용자 부담금 등으로 지출되는 4대 공적연금에 대한 국가부담 규모는 내년 8조7106억원에서 2023년 9조2750억원, 2024년 9조8114억원으로 늘어난 뒤 2025년이 되면 10조4381억원까지 불어날 전망이다. 국가부담 규모가 늘어나는 주된 이유는 수입보다 지출이 많은 적자 때문이다. 내년에 공무원·군인연금은 각각 3조730억원, 2조9077억원의 적자를 기록한다. 사학연금은 2023년부터 적자로 전환한다. 흑자인 국민연금을 제외하면 공무원·군인·사학연금의 재정수지 적자폭은 2023년 8조9128억원, 2024년 9조6832억원, 2025년 11조2498억원으로 급증한다. 국민연금도 안심할 수 없다. 갈수록 연금 적자 규모가 커지고 있어서다. 국회예산정책처가 지난해 내놓은 4대 공적연금 장기전망에 따르면 오는 2050년에는 공무원연금과 군인연금, 사학연금의 적자 규모가 각각 17조2000억원, 4조2000억원, 2조5000억원 수준으로 불어난다. 예정처는 국민연금 역시 오는 2040년이면 14조1000억원 규모의 적자로 전환할 것으로 봤다. 윤석명 한국보건사회연구원 연구위원은 “현재 공무원·군인연금 등 공적연금 체계는 평균수명, 경제여건 변화 등에 따라 연금 지급이 달라지는 자동안전장치가 없다”며 “연금을 받는 사람보다 내는 사람이 훨씬 많았던 당시 세대간 연대 원리에 따라 설계된 구조를 현재 상황에 맞게 조정하는 것이 필요하다”고 말했다. \n",
            "\n",
            "Summary)\n",
            "['[세종=이데일리 원다연 기자] 내년 공무원·군인·국민·사학연금 등 4대 공적연금의 적자 보전 등을 위해 투입되는 국가재정이 8조7000억원에 달할 전망이다', ' 5일 기획재정부가 국회에 제출한 `2021~2025년 국가재정 운용계획`에 따르면 적자 보전과 사용자 부담금 등으로 지출되는 4대 공적연금에 대한 국가부담 규모는 내년 8조7106억원에서 2023년 9조2750억원, 2024년 9조8114억원으로 늘어난 뒤 2025년이 되면 10조4381억원까지 불어날 전망이다', ' 사학연금은 2023년부터 적자로 전환한다']\n",
            "\n",
            "\n",
            "\n",
            "Original)\n",
            "\"전세계 부유식 해상풍력발전에서 최초와 최대 타이틀을 보유하고 있는 기업으로서 한국을 아시아 최고의 해상풍력 클러스터(집적단지) 후보지로 판단하고 사업을 진행 중입니다. 한국은 해상풍력발전에 성공할 수 있는 모든 제반 조건을 갖추고 있습니다.\" 노르웨이 국영 종합 에너지기업 에퀴노르의 한국 지부를 이끌고 있는 쟈크 엔티엔 미셸 에퀴노르코리아 지사장은 지난 2일 머니투데이와 만난 자리에서 부유식 해상풍력 최강자인 에퀴노르가 한국을 사업지로 선택한 이유에 대해 이같이 역설했다. 1972년 설립돼 전 세계 30여 개국에서 40여년간 석유·가스를 개발하고 공급해온 에퀴노르는 기후 변화에 문제의식을 가지면서 10여 년 전부터 풍력·태양광 등 신재생에너지 개발에 눈을 돌려왔다. 특히 부유식 해상풍력발전은 에퀴노르의 전문 분야다. 에퀴노르는 이미 한국 울산에서 800MW(메가와트) 규모 반딧불 부유식 해상풍력발전 사업과 200MW 규모 동해1 부유식 해상풍력발전 사업 개발을 맡고 있다. 신재생에너지 사업 전략통으로 꼽히는 미셸 지사장이 한국 해상풍력발전의 사업성을 검토하기 위해 한국에 처음 발령을 받아 온 것은 2018년이다. 그는 당시 한국이 해상풍력발전 적임지란 판단을 내렸고 2019년 한국지부에 정식으로 부임했다. 미셸 지사장이 해상풍력발전의 성공 요건으로 꼽은 것은 5가지다. △정부의 확고한 의지 △대규모 해상풍력발전 가능 여부 △해상풍력발전 공급망 △충분히 많은 전력 소비량 △연평균 풍속·풍속 분포 등 적당한 풍황자원 등이다. 에퀴노르는 한국이 이 모든 걸 갖췄다고 확신했다. 미셸 지사장은 \"해상풍력발전을 추진하려는 (정부) 당국의 의지가 가장 중요한데 한국은 의지도 확고하고, 해상풍력 규모를 크게 키울 수 있는 환경\"이라며 \"해상풍력발전 구조물 제작 역량을 가진 업체도 많아 공급망도 잘 갖춰져 있다\"고 평가했다. 이어 \"한국은 전력 소비량도 독일과 프랑스와 비슷하다\"며 \"정부 지원 의지가 있고 역량도 있어 신재생에너지 사업성이 있는 국가\"라고 말했다. 한국은 지난해 8월 세계 5대 해상풍력 강국으로 도약하겠다는 '해상풍력 발전방안'을 발표하고 2030년까지 해상풍력 12GW(기가와트)를 보급하겠다는 목표를 내놨다. 미셸 지사장은 \"한국 입장에선 새로운 산업을 키울 수 있는 좋은 기회\"라며 \"한국이 부유식 해상풍력 리더로 자리잡으면 한국 공급업체들이 노하우 갖고 다른 나라에 수출할 수 있다\"고 설명했다. 에퀴노르는 정부가 이 목표를 달성하기 위해선 부유식 해상풍력발전이 반드시 필요하다고 분석했다. 부유식 해상풍력은 보통 해안에서 30~70km 떨어진 바다에 설치된다. 육지에서 먼 바다는 풍속도 높고 안정적이다. 미셸 지사장은 \"수심이 80m 이상인 깊은 바다에 전 세계 해상풍력 자원 80% 이상이 몰려있다\"며 \"수심 깊은 바다에서 풍력발전을 하려면 땅에 고정된 고정식이 아니라 부유식 풍력발전을 해야 한다\"고 강조했다. 실제로 수심이 깊은 한국 바다 특성과 잘 맞을 뿐만 아니라 설비 이용률이 높아 수익성이 높다.부유식 해상풍력발전 글로벌 1위 기업인 에퀴노르가 한국 시장에 큰 관심을 갖고 투자한 이유다. 에퀴노르는 2017년부터 세계 최초의 상용 부유식 풍력발전소인 하이윈드(Hywind) 스코틀랜드 해상풍력발전단지(30MW)를 운영 중이다. 내년부턴 노르웨이에서 세계 최대 규모 부유식 풍력발전소인 하이윈드 탐펜(88MW)을 운영한다. 미셸 지사장은 \"에퀴노르는 40년 이상의 해양 (석유·가스) 프로젝트 개발·운영 경험이 있고, 그간 진행했던 부유식 해상풍력발전 프로젝트를 통해 투자비도 굉장히 많이 낮췄다\"며 \"적은 투자비로 부유식 해상풍력 설치가 가능하고 설계기술과 O&M(운영·유지·보수) 능력 등 전체를 아우르는 밸류체인을 갖췄다\"고 자신했다. 이어 \"북해에서 허리케인 등 열악한 해상 조건에서도 해상풍력발전단지를 운영한 경험이 있다\"며 \"지금 프로젝트를 진행하는 울산 바다도 악천후로 유명한데 한국의 환경에 적합한 하부 구조 설계를 할 때 큰 도움이 될 것\"이라고 말했다. 부유식 해상풍력발전은 풍력 터빈이 바다에 떠 있고 케이블이나 체인만 해저에 계류된 방식이라 어종 등 환경에 미치는 영향도 적다. 해상풍력발전 특성상 바다를 공유하는 지역 어민들과 마찰은 피할 수 없지만, 에퀴노르가 맡은 울산 사업은 주민들을 설득하며 착실하게 진행 중이다. 에퀴노르는 지난달 울산 어민단체인 해상풍력사업어민대책위원회와 반딧불 부유식 해상풍력발전단지 상생협약을 체결한 데 이어 동해1 부유식 해상풍력발전 주민동의서를 전달받았다. 울산 반딧불, 동해1 프로젝트는 현재 풍황 계측을 끝내고 발전사업허가신청(EBL)을 준비 중이다. 2026년 이후 상업운전을 개시한다. 에퀴노르는 아울러 남해안에서 2GW(기가와트) 규모 해상풍력발전단지를 개발할 계획이다. 미셸 지사장은 \"현재 남해안에서 풍황 계측을 준비 중인 부지가 있다\"며 \"부유식과 고정식 해상풍력발전을 병행하는 방식으로 구상하고 있다\"고 밝혔다. 에퀴노르는 해상풍력발전 외에도 태양광과 수소, CCS(탄소포집·저장) 등 다양한 신재생에너지 관련 사업을 진행하고 있다. 2030년까지 회사에서 생산하는 에너지 중 16GW를 신재생에너지로 채우겠다는 목표다. 한국에서도 해상풍력발전 사업이 안정적으로 궤도에 오르면 CCS와 수소사업까지 확장할 방침이다. 미셸 지사장은 \"한국에선 해상풍력발전을 1순위로 하되 기회가 되면 다른 신재생에너지 사업도 확장할 것\"이라며 \"아직 구체적 사업 계획은 없지만 수소 관련 회의에 참석하는 등 한국 신재생에너지 상황을 예의주시하고 있다\"고 말했다. \n",
            "\n",
            "Summary)\n",
            "['\" 노르웨이 국영 종합 에너지기업 에퀴노르의 한국 지부를 이끌고 있는 쟈크 엔티엔 미셸 에퀴노르코리아 지사장은 지난 2일 머니투데이와 만난 자리에서 부유식 해상풍력 최강자인 에퀴노르가 한국을 사업지로 선택한 이유에 대해 이같이 역설했다', ' 1972년 설립돼 전 세계 30여 개국에서 40여년간 석유·가스를 개발하고 공급해온 에퀴노르는 기후 변화에 문제의식을 가지면서 10여 년 전부터 풍력·태양광 등 신재생에너지 개발에 눈을 돌려왔다', ' 에퀴노르는 이미 한국 울산에서 800MW(메가와트) 규모 반딧불 부유식 해상풍력발전 사업과 200MW 규모 동해1 부유식 해상풍력발전 사업 개발을 맡고 있다']\n",
            "\n",
            "\n",
            "\n",
            "Original)\n",
            "[이데일리 김미영 기자] 이르면 이번 주 중으로 빗썸과 코인원, 코빗 등 가상자산(암호화폐) 거래소의 은행 실명계좌 연장 여부가 결정될 전망이다. 이들 거래소는 케이뱅크와 제휴를 맺고 있는 업비트와 함께 국내 4대 거래소로 꼽히는 곳이다. 다만 업비트와는 달리 거래소 신고가 의무화되는 9월24일 이후 은행 실명계좌 발급 재계약 여부가 아직 불투명한 상황이다. 5일 금융권에 따르면 NH농협은행은 빗썸·코인원, 신한은행은 코빗과의 실명확인 계좌 발급 재계약 여부를 이르면 오는 8일 각각 발표할 예정이다. 두 은행은 지난달 말에 가상자산 거래소 3곳에 대해 현장 실사를 포함한 위험평가를 끝낸 것으로 전해졌다. 신한은행 관계자는 “가상화폐 거래소 재계약 문제는 자금세탁 관련 부서까지 모두 합의와 결재가 필요한 사안”이라며 “실사는 지난달 말에 마쳤지만, 현재는 막바지 최종 검토를 하고 있다”고 말했다. 두 은행 모두 계약 연장 여부를 미리 밝히진 않았지만, 거래소들의 금융정보분석원(FIU) 신고 마감기한인 24일이 임박해 재계약 연장을 알릴 가능성이 높게 점쳐진다. 은행과 거래소의 막판 협의에서는 거래소들의 자금세탁 방지 장치 강화가 관건이 될 것이란 관측이 많다. 특히 국제자금세탁방지기구(FATF)가 가상자산 사업자에게 부과한 ‘트래블 룰’ 의무가 아직 국내 사업자들 사이에서는 마련되지 않은 상태여서 은행들이 거래소에 이를 보완할 방안을 요구하고 있다. 트래블 룰이란 가상자산을 한 거래소에서 다른 거래소로 옮길 때 송신을 담당하는 거래소가 자산을 수신하는 거래소에 보내는 사람과 받는 사람의 정보를 제공토록 한 규정이다. 우리나라의 경우 지난 3월 시행된 개정 특정금융정보법에 이러한 트래블 룰 규정을 담았지만 업계의 정보 공유시스템 구축에 시간이 소요돼 실제 적용은 내년 3월 25일까지 1년간 유예돼 있다. 가상자산 거래소 시장의 80%가량을 차지하는 업비트는 이미 지난달 케이뱅크와 실명확인 계좌 발급 계약을 연장키로 했다. 지난달 20일에 FIU 신고를 마쳐 현재 FIU에서 업비트의 영업 허용 여부를 심사 중이다. 추가 신고 거래소가 나오지 않는다면 업비트의 독주가 이어지면서 독과점 부작용이 커질 수 있단 점을 당국도 심사과정에서 고려하리란 전망이다. 이들 대형 거래소를 제외한 중소형 거래소의 경우 은행 실명계좌를 확보하지 못한 채 ISMS(정보보호관리체계) 인증을 확보해 우선 코인간 거래를 하면서 영업을 지속해나갈 공산이 크다. 추석 연휴를 빼면 실질적으로 FIU 신고 마감일이 19일까지로 불과 2주밖에 남지 않았는데, 이 사이에 깐깐한 은행들이 실명계좌를 내어줄 가능성이 별로 없어서다. ISMS는 거래소 신고의 최소요건으로, ISMS 인증만 갖춘 거래소에선 원화 입출금이 불가능하다. 한편 금융당국은 지난달 25일 가상자산거래소 63곳 현황을 파악, 7월 말 현재 가상자산사업자 신고에 필수인 ISMS 인증을 받은 업체가 21곳이라고 발표했다. 나머지 42곳 중에선 18곳이 ISMS 인증 신청 중이었고, 24곳은 ISMS 인증 신청조차 하지 않은 것으로 확인했다. 금융당국은 ISMS 미신청 암호화폐 사업자의 폐업, 영업중단 등에 따른 피해를 우려해 투자자들에 각별한 주의를 당부하고 있다. 금융위원회 관계자는 “사전에 예치금·가상자산을 인출하는 등 선제적인 조치를 취해달라”며 “예치금·가상자산의 인출 요청을 거부·지연하거나, 갑작스러운 영업중단 등의 사례가 생기면 당국에 신고해달라”고 말했다. \n",
            "\n",
            "Summary)\n",
            "[' 이들 거래소는 케이뱅크와 제휴를 맺고 있는 업비트와 함께 국내 4대 거래소로 꼽히는 곳이다', ' 다만 업비트와는 달리 거래소 신고가 의무화되는 9월24일 이후 은행 실명계좌 발급 재계약 여부가 아직 불투명한 상황이다', ' 5일 금융권에 따르면 NH농협은행은 빗썸·코인원, 신한은행은 코빗과의 실명확인 계좌 발급 재계약 여부를 이르면 오는 8일 각각 발표할 예정이다']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:14:18,987 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:14:25,604 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:14:31,014 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:14:37,578 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-09-05 07:14:43,130 INFO] Loading checkpoint from /content/drive/MyDrive/BERT_ex/bert_data/bert_classifier/model_step_6000.pt\n",
            "[2021-09-05 07:14:49,672 INFO] * number of parameters: 177854209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu_rank 0\n",
            "\n",
            "[ soc ]\n",
            "\n",
            "Original)\n",
            "(포항=연합뉴스) 손대성 기자 = 장기간 공사 중단으로 흉물로 방치된 경북 포항 아파트단지가 철거에 들어갔다. 5일 포항시에 따르면 북구 용흥동 금광포란재 아파트 부지 소유권자인 솔빛주택건설 등은 지난 3일 철거를 시작했다. 철거 공사 착공식에는 이강덕 시장과 김정재 국회의원을 비롯해 도의원과 시의원 등이 참석했다. 금광포란재 아파트는 지난 1997년 지하 4층, 지상 15층, 314가구 규모로 주택건설 사업계획 승인이 났다. 이후 여러 차례 사업자 변경을 거쳐 공정률 40% 상태에서 사업 주체가 부도나면서 공사가 중단됐다. 이런 상황에서 토지가 제3자에게 경매 처분돼 권리관계 분쟁이 발생했고 수년간 소송을 거치면서 도심 속 흉물로 방치되어 왔다. 시는 최근 법원 판결을 바탕으로 지난 5월 3일 토지소유자인 솔빛주택 신청에 따라 사업승인을 취소했다. 이강덕 시장은 \"시의 적극적인 노력으로 숙원사업이 해결돼 기쁘고 새로운 명품 아파트 건설로 용흥동이 발전하는 계기가 되기를 바란다\"고 말했다. \n",
            "\n",
            "Summary)\n",
            "[' 5일 포항시에 따르면 북구 용흥동 금광포란재 아파트 부지 소유권자인 솔빛주택건설 등은 지난 3일 철거를 시작했다', ' 금광포란재 아파트는 지난 1997년 지하 4층, 지상 15층, 314가구 규모로 주택건설 사업계획 승인이 났다', ' 이런 상황에서 토지가 제3자에게 경매 처분돼 권리관계 분쟁이 발생했고 수년간 소송을 거치면서 도심 속 흉물로 방치되어 왔다']\n",
            "\n",
            "\n",
            "\n",
            "Original)\n",
            "소득 하위 88%가 1인당 25만 원씩 받는 '코로나 상생 국민지원금(5차 재난지원금)' 접수가 6일부터 시작된다. 가구별로 세대주가 신청했던 지난해와 달리  성인은 개인별로 각각 신청할 수 있으며, 신청 첫 주에는 출생연도 끝자리에 따라 요일별로 순차적으로 접수가 진행된다. 행정안전부에 따르면, 6일 오전 9시부터 온라인에서 국민지원금 지급 대상 여부 조회 및 지급 신청이 가능하다.  오프라인은 이달 13일부터 카드사와 연계된 은행이나 지자체 관할 주민센터에서 조회 및 신청을 할 수 있다. 지난해와 달리 올해는 모든 성인이 개별적으로 신청해야 하며, 미성년자는 세대주가 신청해야 한다. 대상자는 신용·체크카드 충전, 지역사랑상품권, 선불카드 중 선택해 신청할 수 있다. 신용·체크카드 충전 방식을 택할 경우엔 카드사 홈페이지·애플리케이션(앱)과 콜센터 등을 통해서, 지역사랑상품권을 택할 경우엔 주소지 관할 지자체의 지역사랑상품권 홈페이지나 앱에서 신청하면 된다. 혼잡을 막기 위해 온라인 신청 첫 주 평일에만 출생연도 끝자리에 따라 5부제를 적용한다. 9월 6일은 출생연도 끝자리가 1·6인 사람만 신청할 수 있다. 출생연도 끝자리가 2·7인 경우엔 7일, 3·8은 8일, 4·9는 9일, 5·0은 10일에 신청하면 된다. 신용·체크카드, 모바일 서울사랑상품권은 신청 다음 날부터 사용 가능하며, 선불카드는 발급 즉시 사용가능하다. 사용처는 전통시장, 식당, 미용실, 약국, 병원, 프랜차이즈 가맹점 등 지역사랑상품권을 사용할 수 있는 업종으로 국한된다. 쿠팡과 같은 대형 온라인몰, 배달의 민족과 같은 대형 배달앱, 백화점, 대형마트, 유흥업종, 프랜차이즈 직영점, 홈쇼핑 등에선 사용할 수 없다. 구체적인 사용처 정보는 별도 홈페이지(국민지원금사용처.kr)이나 네이버지도, 카카오맵 등에서 확인할 수 있다. 지원금 신청 기한은 오는 10월 29일까지이며, 사용 기한은 12월 31일까지다. 기간 내 신청하지 않거나 사용하지 않은 잔액은 모두 환수된다. \n",
            "\n",
            "Summary)\n",
            "[\"소득 하위 88%가 1인당 25만 원씩 받는 '코로나 상생 국민지원금(5차 재난지원금)' 접수가 6일부터 시작된다\", ' 행정안전부에 따르면, 6일 오전 9시부터 온라인에서 국민지원금 지급 대상 여부 조회 및 지급 신청이 가능하다', '  오프라인은 이달 13일부터 카드사와 연계된 은행이나 지자체 관할 주민센터에서 조회 및 신청을 할 수 있다']\n",
            "\n",
            "\n",
            "\n",
            "Original)\n",
            "정부가 코로나19 백신 접종 속도를 높이기 위해 접종 완료자에 대한 추가 혜택을 검토하고 있다. 백신 접종자는 사적 모임 인원에서 제외되는 혜택을 주지만, 이보다 더 큰 혜택을 줘 18~49세 청장년층의 접종률을 대폭 늘리겠다는 복안으로 보인다. 이날 0시 기준으로 18∼49세 청장년층의 예약률은 72.2%로, 전체 대상자 1천 408만 5068명 가운데 1016만 9137명이 예약을 마쳤다. 전해철 중앙재난안전대책본부(중대본) 2차장은 5일 중대본 회의 모두발언에서 “1차 접종을 완료한 국민은 오늘 오전 3000만명(58.4%)을 넘어섰다”며 “추석 전 3600만명에 대한 1차 접종은 물론 10월 중 전 국민의 70%에 대한 접종이 완료되도록 계획에 따른 백신 도입 등에 최선을 다하겠다”고 밝혔다. 전 2차장은 또 “접종률 확대에 따른 방역수칙 일부 완화 등 접종 완료자에 대한 추가 혜택 확대도 검토해 나가겠다”며 “특별히 정부는 백신 접종 속도를 보다 높일 수 있도록 백신 도입 시점부터 전국에 소재한 접종 현장까지의 배송기간을 최소화하는 방안을 강구하고 있다”고 설명했다. 정부는 백신 수송차량을 40대 추가로 투입하고, 토요일 배송 시간을 기존 오후 3시에서 6시로 3시간 연장하기로 했다. 이를 통해 백신 도입 후 3일 이내에 현장 배송이 완료될 수 있도록 할 방침이다. 전 2차장은 코로나19 유행 상황에 대해서는 “오늘 확진자 수는 일요일 기준으로는 지난 8월 8일 이후 가장 낮은 수치(1490명)를 나타냈다”며 “최근 일주일간 국내 발생 일평균 확진자 수는 1671명으로, 8월 둘째 주 이후 3주째 지속 감소세를 보이고 있다”고 분석했다. 그는 또 “감염 재생산지수 역시 3주 연속 감소하고 있는 가운데 2주 연속 1 이하의 수치(0.98)를 나타내고 있다”고 덧붙였다. 그러면서 “내일부터 방역수칙 일부 완화가 포함된 현행 거리두기(수도권 4단계, 비수도권 3단계)가 4주 연장돼 시행된다”며 철저한 방역수칙 준수를 당부했다. 전 2차장은 “내일부터는 거리두기 3단계가 적용되고 있는 지역의 초·중·고교에서 전면등교가 시행된다”며 “안정적 등교와 차질 없는 학사 운영이 이뤄지도록 학교방역 관리에 만전을 기하겠다”고 강조했다. \n",
            "\n",
            "Summary)\n",
            "[' 백신 접종자는 사적 모임 인원에서 제외되는 혜택을 주지만, 이보다 더 큰 혜택을 줘 18~49세 청장년층의 접종률을 대폭 늘리겠다는 복안으로 보인다', '정부가 코로나19 백신 접종 속도를 높이기 위해 접종 완료자에 대한 추가 혜택을 검토하고 있다', ' 이날 0시 기준으로 18∼49세 청장년층의 예약률은 72']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZXpBjfe_Ibc"
      },
      "source": [
        "# 카카오톡으로 연결하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic0zYza5_IYX"
      },
      "source": [
        "# REST API 키\t --> 504d0e40f7bc274e44568959ace57bb3\n",
        "# code         --> adrGV5gltu1PxQPz4WzAq0bdxM9Cv711DHx-R-Fmd2o0lTGU32rKZRuFiw3ONh8O9mgqygorDKcAAAF7sgaedw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eybOmARb_IVD"
      },
      "source": [
        "import requests\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58acPYS9lBhC"
      },
      "source": [
        "rest_api_key = \"504d0e40f7bc274e44568959ace57bb3\"\n",
        "redirect_url = \"https://localhost:3000\"\n",
        "code = \"koYt5b2USnoiUwQal47wSbHB6HHtW3qJItsV5Ls5bMuk415x_kfjnawjZOzW7oy37XJ6Cgo9cxcAAAF7tPJejQ\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU53SIOblBPF",
        "outputId": "607d4248-18e5-4f1b-d064-d34c87820b88"
      },
      "source": [
        "# 카카오톡 메세지 API\n",
        "url = 'https://kauth.kakao.com/oauth/token'\n",
        "\n",
        "data = {\n",
        "    \"grant_type\" : \"authorization_code\",\n",
        "    \"client_id\" : rest_api_key,\n",
        "    \"redirect_uri\" : redirect_url,\n",
        "    \"code\"         : code\n",
        "}\n",
        "\n",
        "response = requests.post(url, data=data)\n",
        "tokens = response.json()\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': '1KHJK4cXr9tBpT57gh7ndFfSXny3igsmx_YGlwo9dRoAAAF7tPMy_Q', 'token_type': 'bearer', 'refresh_token': 'r47FrYwGy-Em0tTV1PirKdZx8T4HpRzc1MBWHQo9dRoAAAF7tPMy_Q', 'expires_in': 21599, 'scope': 'talk_message', 'refresh_token_expires_in': 5183999}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljzOIqJJlBLr"
      },
      "source": [
        "# 토큰을 json파일로 저장\n",
        "with open(\"/content/drive/MyDrive/BERT_ex/kakao_connect/kakao_token.json\", \"w\") as fp:\n",
        "    json.dump(tokens, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3zwPhkNlBIn"
      },
      "source": [
        "# 카카오톡으로 기사 보내기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uEtmoS5lBFw"
      },
      "source": [
        "# 필요한 라이브러리 \n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlDus7-slA8V"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import requests\n",
        "\n",
        "# 저장하는 함수\n",
        "def save_tokens(filename, tokens):\n",
        "    with open(filename, \"w\") as fp:\n",
        "        json.dump(tokens, fp)\n",
        "\n",
        "# 읽어오는 함수\n",
        "def load_tokens(filename):\n",
        "    with open(filename) as fp:\n",
        "        tokens = json.load(fp)\n",
        "        \n",
        "    return tokens\n",
        "\n",
        "# refresh_token으로 access_token 갱신하는 함수\n",
        "def update_tokens(app_key, filename) : \n",
        "    tokens = load_tokens(filename)\n",
        "\n",
        "    url = \"https://kauth.kakao.com/oauth/token\"\n",
        "    data = {\n",
        "        \"grant_type\" : \"refresh_token\",\n",
        "        \"client_id\"  : app_key,\n",
        "        \"refresh_token\" : tokens['refresh_token']\n",
        "    }\n",
        "    response = requests.post(url, data=data)\n",
        "\n",
        "    # 요청에 실패했다면,\n",
        "    if response.status_code != 200:\n",
        "        print(\"error! because \",  response.json())\n",
        "        tokens = None\n",
        "    else: # 성공했다면,\n",
        "        print(response.json())\n",
        "        # 기존 파일 백업\n",
        "        now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_filename = filename+\".\"+ now\n",
        "        os.rename(filename, backup_filename)\n",
        "        # 갱신된 토큰 저장\n",
        "        tokens['access_token'] = response.json()['access_token']\n",
        "        save_tokens(filename, tokens)\n",
        "        \n",
        "    return tokens\n",
        "\n",
        "# 메시지 전송 함수\n",
        "def send_message(filename, template):\n",
        "    tokens = load_tokens(filename)\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": \"Bearer \" + tokens['access_token']\n",
        "    }\n",
        "    \n",
        "    # JSON 형식 -> 문자열 변환\n",
        "    payload = {\n",
        "        \"template_object\" : json.dumps(template)\n",
        "    }\n",
        "\n",
        "    # 카카오톡 보내기\n",
        "    url = \"https://kapi.kakao.com/v2/api/talk/memo/default/send\"\n",
        "    res = requests.post(url, data=payload, headers=headers)\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va56RreU0Tac",
        "outputId": "4d16c3ca-e17b-4c63-fc09-97062e0bbc4a"
      },
      "source": [
        "#token이 저장된 파일\n",
        "KAKAO_TOKEN_FILENAME = \"/content/drive/MyDrive/BERT_ex/kakao_connect/kakao_token.json\"\n",
        "KAKAO_APP_KEY = \"504d0e40f7bc274e44568959ace57bb3\"\n",
        "update_tokens(KAKAO_APP_KEY, KAKAO_TOKEN_FILENAME)  # 아직은 토큰이 유효하다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': 'q7QgZKZdPphyEgHfQ2ymsSATVuGVy5PQ5sNYGgo9dJkAAAF7tPNo4A', 'token_type': 'bearer', 'expires_in': 21599}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'access_token': 'q7QgZKZdPphyEgHfQ2ymsSATVuGVy5PQ5sNYGgo9dJkAAAF7tPNo4A',\n",
              " 'expires_in': 21599,\n",
              " 'refresh_token': 'r47FrYwGy-Em0tTV1PirKdZx8T4HpRzc1MBWHQo9dRoAAAF7tPMy_Q',\n",
              " 'refresh_token_expires_in': 5183999,\n",
              " 'scope': 'talk_message',\n",
              " 'token_type': 'bearer'}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "954UaoFJlA2k"
      },
      "source": [
        "# 분야 지정\n",
        "my_section = \"eco\"\n",
        "\n",
        "# 사용자가 선택한 카테고리를 제목에 넣기 위한 dictionary\n",
        "sections_ko = {'pol': '정치', 'eco' : '경제', 'soc' : '사회'}\n",
        "\n",
        "# 네이버 뉴스 URL\n",
        "navernews_url = \"https://news.naver.com/main/home.naver\"\n",
        "\n",
        "# 추후 각 리스트에 들어갈 내용(content) 만들기\n",
        "contents = []\n",
        "\n",
        "# 리스트 템플릿 형식 만들기\n",
        "template = {\n",
        "    \"object_type\" : \"list\",\n",
        "    \"header_title\" : sections_ko[my_section] + \" 분야 상위 뉴스 빅3\",\n",
        "    \"header_link\" : {\n",
        "        \"web_url\": navernews_url,\n",
        "        \"mobile_web_url\" : navernews_url\n",
        "    },\n",
        "    \"contents\" : contents,\n",
        "    \"button_title\" : \"네이버 뉴스 바로가기\"\n",
        "}\n",
        "\n",
        "## 내용 만들기\n",
        "# 각 리스트에 들어갈 내용(content) 만들기\n",
        "for news_info in news_dic[my_section]:  # 위에서 구한 news_dic에서 my_section에서 지정한 분야를 가져온다.\n",
        "    content = {\n",
        "        \"title\" : news_info.get('title'),\n",
        "        \"image_url\" : news_info.get('image_url'),\n",
        "        \"image_width\" : 50, \"image_height\" : 50,\n",
        "        \"link\": {\n",
        "            \"web_url\": news_info.get('news_url'),\n",
        "            \"mobile_web_url\": news_info.get('news_url')\n",
        "        }\n",
        "    }\n",
        "\n",
        "    contents.append(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfmFl0xTB0dO",
        "outputId": "29a676f5-e8a2-4aa4-8376-50ddbeb9e107"
      },
      "source": [
        "# 카카오톡 메시지 전송\n",
        "res = send_message(KAKAO_TOKEN_FILENAME, template)\n",
        "\n",
        "if res.json().get('result_code') == 0:\n",
        "    print('뉴스를 성공적으로 보냈습니다.')\n",
        "else:\n",
        "    print('뉴스를 성공적으로 보내지 못했습니다. 오류메시지 : ', res.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뉴스를 성공적으로 보냈습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qynye6xgDD5T",
        "outputId": "506548be-f62a-47a4-a04b-5b9b3f45bab6"
      },
      "source": [
        "# 각 뉴스의 요약 결과를 전송\n",
        "for idx, news_info in enumerate(news_dic[my_section]):\n",
        "    # 텍스트 템플릿 형식 만들기\n",
        "    template = {\n",
        "        \"object_type\": \"text\",\n",
        "        \"text\": '# 제목 : ' + news_info.get('title') + \\\n",
        "                '\\n\\n# 요약 : ' + news_info.get('news_contents_summ')[0],  # 기사요약 정보가 들어간다.\n",
        "        \"link\": {\n",
        "            \"web_url\": news_info.get('news_url'),\n",
        "            \"mobile_web_url\": news_info.get('news_url')\n",
        "        },\n",
        "        \"button_title\": \"자세히 보기\"\n",
        "    }\n",
        "    \n",
        "    # 카카오톡 메시지 전송\n",
        "    res = send_message(KAKAO_TOKEN_FILENAME, template)\n",
        "    if res.json().get('result_code') == 0:\n",
        "        print('뉴스를 성공적으로 보냈습니다.')\n",
        "    else:\n",
        "        print('뉴스를 성공적으로 보내지 못했습니다. 오류메시지 : ', res.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뉴스를 성공적으로 보냈습니다.\n",
            "뉴스를 성공적으로 보냈습니다.\n",
            "뉴스를 성공적으로 보냈습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8UIY70EDD1W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}